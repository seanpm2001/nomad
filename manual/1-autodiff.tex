\chapter{The Theory of \\ Automatic Differentiation}

Automatic differentiation is a powerful computation tool for the exact, at least up to 
arithmetic precision, evaluation of linear differential operators on the elaborate, 
composite functions common to practical applications.  In particular, automatic 
differentiation techniques factor a linear differential operator into analytic calculations, 
which can be explicitly implemented, from the algebraic manipulations that can be 
programmatically automated.

In this chapter we review the theory behind linear differential operators, dual numbers,
and, ultimately, automatic differentiation.  We begin by considering differential operators
in a formal setting, in particular the spaces on which they act, before introducing dual
numbers and showing how they compliment these operators.  Last we integrate both
perspectives into automatic differentiation and its implementation.

\section{Linear Differential Operators}

When considering a one-dimensional function, $f : \mathbb{R} \rightarrow \mathbb{R}$, 
a common notion is that the derivative defines a ``best'' linear approximation
to the function in a neighborhood around any point, $x_{0} \in \mathbb{R}$,
%
\begin{equation*}
f \! \left( x \right) - f \! \left( x_{0} \right)
\approx \frac{ \mathrm{d} f }{ \mathrm{d} x } \! \left( x_{0} \right) \left( x - x_{0} \right).
\end{equation*}
%
A more revealing interpretation, however, arises when we consider
$\delta x = x - x_{0}$ and $\delta f = f \! \left( x \right) - f \! \left( x_{0} \right)$ as
perturbations, in which case the derivative is just a linear map that propagates
an input perturbation into an output perturbation,
%
\begin{equation*}
\delta f = \frac{ \mathrm{d} f }{ \mathrm{d} x } \! \left( x_{0} \right) \delta x.
\end{equation*}

Linear differential operators formalize this intuition of propagating perturbations
through functions.  Any formal treatment, however, first requires a much more
careful definition of perturbations, especially for multivariate functions.  In this
section we review the necessary theory of first-order linear differential operators
and then briefly discuss the generalization to higher-order operations.

\subsection{First-Order Linear Differential Operators}

First-order linear differential operators generalize the one-dimensional construction
above.  The most significant complication is that, in general, there are actually two 
kinds of first-order perturbations: \textit{tangents} and \textit{sensitivities}.

\subsubsection{Tangents}

A defining property of perturbations is that they can be added together and scaled,
in other words they form a vector space.  Given the real space $\mathbb{R}^{N}$,
the $N$-dimensional vector space of perturbations at every point, $x \in \mathbb{R}^{N}$, 
is called the \textit{tangent space} and denoted $T_{x} \mathbb{R}^{N}$.  The notation
comes from interpreting first-order perturbations as tangent vectors of curves
passing through $x$.

Consequently, given a smooth function $f : \mathbb{R}^{N} \rightarrow \mathbb{R}^{M}$
a first-order linear differential operator is a linear map from the tangent space of
the input to the tangent space of the output,
%
\begin{equation*}
D : T_{x} \mathbb{R}^{N} \rightarrow T_{ f ( x ) } \mathbb{R}^{M}.
\end{equation*}
%
Taking the standard bases in both tangent spaces, we can identify any tangent,
$\delta x \in T_{x} \mathbb{R}^{N}$, by its components, $\delta x_{n}$.  In this case
the operator $D$ is equivalent to an $m \times n$ matrix,
%
\begin{equation*}
\delta f_{m} = \sum_{n = 1}^{N} D_{mn} \, \delta x_{n} \in T_{f(x)} \mathbb{R}^{M}.
\end{equation*}

One particularly important first-order linear differential operator is the \textit{pushforward}
or \textit{tangent map} canonically defined for any function,
%
\begin{equation*}
f_{*} : T_{x} \mathbb{R}^{N} \rightarrow T_{ f ( x ) } \mathbb{R}^{M}.
\end{equation*}
%
In components the pushforward is given exactly by the Jacobian matrix of partial derivatives,
%
\begin{equation*}
\left( f_{*} \right)_{mn} = \frac{ \partial f_{m} }{ \partial x_{n} }.
\end{equation*}

Pushforwards behave well under function composition.  Given two functions,
$f : \mathbb{R}^{N} \rightarrow \mathbb{R}^{L}$ and
$g : \mathbb{R}^{L} \rightarrow \mathbb{R}^{M}$, the pushforward of their compositions
is simply
%
\begin{equation*}
\left( g \circ f \right)_{*} = g_{*} \cdot f_{*},
\end{equation*}
%
or, in components,
%
\begin{align*}
\left( \left( g \circ f \right)_{*} \right)_{mn} &= 
\sum_{l = 1}^{L} \left( g_{*} \right)_{ml} \left( f_{*} \right)_{ln}
\\
\frac{ \partial g_{m} }{ \partial x_{n} } &= 
\sum_{l = 1}^{L} \frac{ \partial g_{m} }{ \partial f_{l} } \cdot \frac{ \partial f_{l} }{ \partial x_{n} },
\end{align*}
%
which is just the chain rule.  Because of this behavior, the pushforward of a
composite function is straightforward computationally, provided that the pushforwards
of each component function are known.  In particular, using the standard basis the
composition pushforward is given simply by a succession of vector-matrix products.

\subsubsection{Sensitivities}

An immediate consequence of this more formal perspective is the introduction of a second
kind of first-order perturbation.  The space of linear operators acting on tangents,
%
\begin{equation*}
\alpha : T_{x} \mathbb{R}^{N} \rightarrow \mathbb{R},
\end{equation*}
%
is itself a vector space, known as the \textit{cotangent space} and denoted 
$T^{*}_{x} \mathbb{R}^{N}$.  Composing these maps with a first-order linear differential 
operator assigns a value to each tangent,
%
\begin{equation*}
\alpha \circ D : T_{x} \mathbb{R}^{N} \rightarrow \mathbb{R},
\end{equation*}
%
in some sense quantifying how sensitive the underlying function is to a given 
perturbation.  Consequently elements of the cotangent space are often known as
\textit{sensitivities} and provide dual perturbation to the tangents.

Any first-order linear differential operator,
$D : T_{x} \mathbb{R}^{N} \rightarrow T_{ f ( x ) } \mathbb{R}^{M}$, acting on
the tangent spaces canonically defines an \textit{adjoint operator} that acts on the 
cotangent spaces,
%
\begin{equation*}
D^{*} : T^{*}_{f (x) } \mathbb{R}^{M} \rightarrow T^{*}_{x} \mathbb{R}^{N}.
\end{equation*}
%
The matrix representation of the adjoint operator is, perhaps unsurprisingly, 
simply the adjoint of the matrix representation of the original operator,
%
\begin{equation*}
\mathrm{d} x_{n} = \sum_{m = 1}^{M} D_{nm} \mathrm{d} f_{m},
\end{equation*}
%
where $\mathrm{d} x \in T^{*}_{x} \mathbb{R}^{N}$ and
$\mathrm{d} f \in T^{*}_{x} \mathbb{R}^{M}$.

The adjoint of the pushforward is known as the \textit{pullback},
%
\begin{equation*}
f^{*} : T^{*}_{f (x) } \mathbb{R}^{M} \rightarrow T^{*}_{x} \mathbb{R}^{N}.
\end{equation*}
%
Under the composition,
%
\begin{equation*}
\left( g \circ f \right)^{*} = f^{*} \cdot g^{*}
\end{equation*}
%
or, in components,
%
\begin{equation*}
\left( \left( g \circ f \right)^{*} \right)_{mn} = 
\sum_{l = 1}^{L} \left( g^{*} \right)_{ml} \left( f^{*} \right)_{ln}.
\end{equation*}
%
Computing the input sensitivities corresponding to the output sensitivities then reduces
to vector-matrix products moving backwards, against the flow of the function evaluation.

\subsection{Higher-Order Linear Differential Operators}

Unfortunately, a theory of higher-order perturbations, and the corresponding
linear differential operators, is substantially more complicated, requiring an elegant but
abstract construction known as \textit{jet spaces} to generalize the tangent space.  
For example, a second-order linear differential operator does simply map second-order 
tangents to second-order tangents but also requires \textit{two} first-order tangents,
%
 \begin{equation*}
\delta^{2} f_{m} 
= \sum_{n = 1}^{N} \frac{ \partial f_{m} }{ \partial x_{n} } \delta^{2} x_{n} 
+ \sum_{n = 1}^{N} \sum_{n' = 1}^{N} 
\frac{ \partial^{2} f_{m} }{ \partial x_{n} \partial x_{n'} } 
\delta x_{n} \delta x_{n'}.
\end{equation*}
%
Moreover, adjoint operators do not immediately generalize -- at higher-orders adjoint
maps require the interaction of tangents and sensitivities.

Fortunately we do not have to consider these higher-order jet spaces directly
because the introduction of \textit{dual numbers} provides an alternative means
of computing how higher-order perturbations propagate across a function.

\section{Dual Numbers}

Dual numbers are a generalization of real numbers that naturally unify the
action of a function, its pushforward, and, with some care, its pullback;
consequently they are the ideal basis for implementing automatic differentiation. 
In this section we derive first-order dual numbers and then consider the construction 
of higher-order dual numbers and higher-order linear differential operators.

\subsection{First-Order Dual Numbers}

Dual numbers extend the real numbers, $\mathbb{R}$, with the
addition of a new element, $\mathfrak{a}$, which is nilpotent, $\mathfrak{a}^{2} = 0$,
and denoted a \textit{dual unit}.

This extended space of dual numbers forms a two-dimensional associative algebra 
over $\mathbb{R}$.  In other words, the addition of the dual unit generates a two-dimensional
space, $\mathbb{D}$, comprised of elements,
%
\begin{equation*}
\xi = x + \mathfrak{a} \, \delta x; \, \xi \in \mathbb{D}, \, x, \delta x \in \mathbb{R},
\end{equation*}
%
that are naturally equipped with associative addition and multiplication operations,
%
\begin{align*}
\xi_{1} + \xi_{2} 
&=
 \left( x_{1} + \mathfrak{a} \, \delta x_{1} \right) 
 + \left( x_{2} + \mathfrak{a} \,  \delta x_{2} \right) 
\\
&=
\left( x_{1} + x_{2} \right) 
+ \mathfrak{a} \left( \delta x_{1} + \delta x_{2} \right) 
\\
\xi_{1} \cdot \xi_{2} 
&= 
\left( x_{1} + \mathfrak{a} \, \delta x_{1} \right) 
\cdot \left( x_{2} + \mathfrak{a} \, \delta x_{2} \right) 
\\
&= 
x_{1} x_{2} + \left( x_{1} \delta x_{2} 
+ \delta x_{1} x_{2} \right) \mathfrak{a} 
+ \delta x_{1} \delta x_{2} \, \mathfrak{a}^{2} 
\\
&= 
x_{1} x_{2} + \left( x_{1} \delta x_{2} + \delta x_{1} x_{2} \right) \mathfrak{a}.
\end{align*}  

Any smooth function $f : \mathbb{R} \rightarrow \mathbb{R}$ induces a function on 
dual numbers, $f : \mathbb{D} \rightarrow \mathbb{D}$, via a Taylor series around any 
purely real point, $\xi_{0} = x_{0} + \mathfrak{a} \, 0$,
%
\begin{align*}
f \! \left( \xi \right) 
&= 
\sum_{n = 0}^{\infty} \frac{ \left( \xi - \xi_{0} \right)^{n} }{n!} 
\frac{ \partial^{n} f }{ \partial \xi^{n} } \! \left( \xi_{0} \right) 
\\
&= 
\sum_{n = 0}^{\infty} \frac{ \left( \left( x - x_{0} \right) + \mathfrak{a} \, \delta x \right)^{n} }{n!} 
\frac{ \partial^{n} f }{ \partial x^{n} } \! \left( x_{0} \right)
\\
&= 
\sum_{n = 0}^{\infty} \frac{ \left( x - x_{0} \right)^{n} }{n!} 
\frac{ \partial^{n} f }{ \partial x^{n} } \! \left( x_{0} \right)
+ \mathfrak{a} \, \delta x \sum_{n = 0}^{\infty} \frac{ \left( x - x_{0} \right)^{n - 1} }{\left( n - 1 \right)!} 
\frac{ \partial^{n} f }{ \partial x^{n} } \! \left( x_{0} \right) 
\\
&= 
f \! \left( x \right) 
+ \mathfrak{a} \, \delta x \frac{ \partial f }{ \partial x } \! \left( x \right).
\end{align*}

Generalizing to multivariate functions is straightforward: any function 
$f : \mathbb{R}^{N} \rightarrow \mathbb{R}^{M}$ is first decomposed into $M$
functions, $f_{m} : \mathbb{R}^{N} \rightarrow \mathbb{R}$, which extend
to dual numbers as
%
\begin{equation} \label{dual_function}
f_{m} \! \left( \mbox{\boldmath{$\xi$}} \right) = f_{m} \! \left( \mathbf{x} \right) 
+ \mathfrak{a} \sum_{n = 1}^{N} 
\delta x_{n} \frac{ \partial f_{m} }{ \partial x_{n} } \! \left( \mathbf{x} \right).
\end{equation}
%
If we consider the dual component as identifying an element of the tangent
space, then the dual part of the extended function is exactly the pushforward 
of the original function.  

Similarly, we can interpret the dual component as a sensitivity.  In this case
the action of a function on the dual number is given by the adjoint action of the
pushforward,
%
\begin{equation} \label{dual_function}
\xi_{n}  = x_{n}  + \mathfrak{a} \sum_{m = 1}^{M} 
\delta f_{m} \frac{ \partial f_{m} }{ \partial x_{n} } \! \left( \mathbf{x} \right).
\end{equation} 

Because dual numbers inherently integrate a function with its pushforward or
pullback, they are the natural type from which we can implement linear
differential operators and, consequently, automatic differentiation.

\subsection{Higher-Order Dual Numbers}

The naturalness of dual numbers also generalizes to higher-order perturbations
with the introduction of multiple dual units, providing an means of computing
higher-order linear differential operators without having to introduce the mathematical
machinery of jet spaces.

At first-order, the real and dual parts of a first-order dual number are real numbers,
%
\begin{equation*}
\zeta = z + \mathfrak{a} \, \delta z, \, z, \delta z \in \mathbb{R}.
\end{equation*}
%
If we introduce a second dual unit, $\mathfrak{b}$, then we can construct a second-order 
dual number by replacing the real-valued parts of $\zeta$ with dual numbers,
%
\begin{alignat*}{3}
\zeta
&=
\;\, \xi
&&+ \mathfrak{a} \;\;\, \eta
\\
&=
\left( x + \mathfrak{b} \, \delta x \right)
&&+ \mathfrak{a} \left( \delta y + \mathfrak{b} \, \delta^{2} y \right), 
\, x, \delta x, \delta y, \delta^{2} y \in \mathbb{R}.
\end{alignat*}
%
From the linear differential operator perspective, $x$ is a the value, $\delta x$ and 
$\delta y$ are first-order perturbations, and $\delta^{2} y$ is a second-order perturbation.
Higher-order dual numbers follow recursively with the introduction of a new dual unit
and the replacement of real-valued components with new dual numbers.  An $n$-th 
order dual number, for example, is constructed from $n$ distinct dual units and
$2^{n}$ real components.

When constructing higher-order linear differential operators, these components may 
be considered as both higher-order tangents and higher-order sensitivities. Consequently 
it's convenient to define notation agnostic to the specific map being considered.
We will refer to the components of each bottem-level dual number as \textit{values} and
\textit{gradients}; for example, at second-order $x$ would be a first-order value, $\delta x$ 
a first-order gradient, $\delta y$ a second-order value, and $\delta^{2} y$ as the second-order 
gradient.

Extending functions to higher-order dual numbers, and constructing higher-order
linear differential operators in the process, also proceeds recursively.  Here we
explicitly evaluate the extension to second and third-order dual numbers.

\subsubsection{Second-Order}

Given the second-order dual number,
%
\begin{alignat*}{3}
\zeta_{i} 
&=
\xi_{i} 
&&+ \mathfrak{a} \, \eta_{i}
\\
&=
\left( x_{i} + \mathfrak{b} \, \delta x_{i} \right)
&&+ \mathfrak{a} \left( \delta y_{i} + \mathfrak{b} \, \delta^{2} y_{i} \right),
\end{alignat*}
%
a second-order dual number valued-function, evaluates to
%
\begin{align*}
f_{i} \! \left(  \mbox{\boldmath{$\zeta$}} \right)
=& \,
f_{i} \! \left(  \mbox{\boldmath{$\xi$}} \right) 
+ \mathfrak{a} \sum_{j} \eta_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left(  \mbox{\boldmath{$\xi$}} \right) 
\\
=& \,
f_{i} \! \left( \mathbf{x} + \mathfrak{b} \, \delta \mathbf{x} \right) 
+ \mathfrak{a} \sum_{j} \left( \delta y_{j} + \mathfrak{b} \, \delta^{2} y_{j} \right)
\frac{ \partial f_{i} }{ \partial x_{j} } 
\! \left( \mathbf{x} + \mathfrak{b} \, \delta \mathbf{x} \right) 
\\
=& \,
f_{i} \! \left( \mathbf{x} \right) 
+ \mathfrak{b} \sum_{j} \delta x_{j}  \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x} \right)
\\
&+ 
\mathfrak{a} \sum_{j} \left( \delta y_{j} + \mathfrak{b} \, \delta^{2} y_{j} \right)
\left( \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x}\right) + 
\mathfrak{b} \, \sum_{k} \delta x_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{x} \right) \right) 
\\
=&
\quad\quad\quad 
f_{i} \! \left( \mathbf{x} \right) 
\quad\quad\quad\;\;\;
+ \mathfrak{b} \;\;\;
\sum_{j} \delta x_{j}  \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x} \right)
\\
&+ 
\mathfrak{a} \left( 
\sum_{j} \delta y_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x} \right) 
+ \mathfrak{b} \left(
\sum_{j} \delta^{2} y_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x} \right)
+ \sum_{jk} \delta x_{k} \, \delta y_{j}
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{x} \right)
\right)
\right)
\end{align*}
%
These results are summarized in Table \ref{tab:secondOrder}.

\begin{table*}[t!]
	\centering
	\renewcommand{\arraystretch}{2}
	\begin{tabular}{ccc}
	\rowcolor[gray]{0.9} \textbf{Component} & \textbf{Input} & \textbf{Output} \\
	First Value & 
	$\mathbf{x}$ & 
	$f_{i} \! \left( \mathbf{x} \right)$ 
	\\
	\rowcolor[gray]{0.9} 
	First Gradient & 
	$\delta \mathbf{x}$ &
	$\displaystyle \sum_{j} \delta x_{j}  \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x} \right)$
	\\
	Second Value & 
	$\delta \mathbf{y} $ & 
	$\displaystyle \sum_{j} \delta y_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x} \right)$
	\\
	\rowcolor[gray]{0.9} 
	Second Gradient & 
	$\delta^{2} \mathbf{y} $ & 
	$\displaystyle \sum_{j} \delta^{2} y_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x} \right)
	+ \sum_{jk} \delta x_{k} \, \delta y_{j}
	\frac{ \partial f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{x} \right)$
	\\
	\end{tabular}
	\caption{Recursively expanding a function with respect to a second-order dual number 
	gives each component of the function's second-order pushforward.
	\label{tab:secondOrder}}
\end{table*}

Considering the first-order gradient, second-order value, and second-order gradient as tangents 
immediately defines two first-order linear differential operators and one second-order linear 
differential operator.  Unfortunately the same is not true if we try to consider all three components
as sensitivities, as the second term in the second-order gradient map no longer makes
sense.  

The generalization of an adjoint map does not consider all components as sensitivities but rather
considers all $k$th-order values as $(k - 1)$-order tangents (with a $0$-order tangent
taken to be an input value) and all $k$th-order gradients as $k$th-order sensitivities.  Once the partial
derivatives are contracted against the tangents they can be transposed to give the
components of the pullback map.  For example, given our usual test function 
$f : \mathbb{R}^{N} \rightarrow \mathbb{R}^{M}$ with the perturbations 
$\delta y \rightarrow \mathrm{d} f$ and $\delta^{2} y \rightarrow \mathrm{d}^{2} f$, the 
second-order pullback is given by
%
\begin{equation*}
\mathrm{d}^{2} x_{n}
= \sum_{m = 1}^{M} \mathrm{d}^{2} f_{m} \frac{ \partial f_{m} }{ \partial x_{n} }
+ \sum_{m = 1}^{M} \mathrm{d} f_{m} 
\left[ \sum_{n'=1}^{N} \delta x_{n'} \frac{ \partial^{2} f_{m} }{ \partial x_{n'} \partial x_{n} } \right].
\end{equation*}

\subsubsection{Third-Order}

Continuing to third-order follows in kind.  Given a third-order dual number,
%
\begin{alignat*}{10}
\zeta_{i} 
&=
&& \xi_{i} && && &&
&&+ \mathfrak{a} \, 
&& \eta_{i} && && &&
\\
&=
( && \sigma_{i} && + \mathfrak{b} && \tau_{i} &&)
&&+ \mathfrak{a} \,
( && \upsilon_{i} && + \mathfrak{b} && \nu_{i} && )
\\
&=
(( && s_{i} + \mathfrak{c} \, \delta s_{i} )
&& + \mathfrak{b} \;
( && \delta t_{i} + \mathfrak{c} \, \delta^{2} t_{i}  ) &&)
&&+ \mathfrak{a} \;
(( && \delta u_{i} + \mathfrak{c} \, \delta^{2} u_{i} )
&& + \mathfrak{b} \;
( && \delta^{2} v_{i} + \mathfrak{c} \, \delta^{3} v_{i} ) &&)
\end{alignat*}
%
we have
%
\begin{align*}
f_{i} \! \left( \mbox{\boldmath{$\zeta$}} \right)
%
=& \,
f_{i} \! \left( \mbox{\boldmath{$\xi$}}\right) 
+ \mathfrak{a} \sum_{j} \eta_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mbox{\boldmath{$\xi$}} \right) 
\\
%
=& \,
f_{i} \! \left( \mbox{\boldmath{$\sigma$}} + \mathfrak{b} \, \mbox{\boldmath{$\tau$}} \right) 
+ \mathfrak{a} \sum_{j} \left( \upsilon_{j} + \mathfrak{b} \, \nu_{j} \right)
\frac{ \partial f_{i} }{ \partial x_{j} } 
\! \left( \mbox{\boldmath{$\sigma$}} + \mathfrak{b} \, \mbox{\boldmath{$\tau$}} \right) 
\\
%
=& \,
f_{i} \! \left( \mbox{\boldmath{$\sigma$}} \right) + \mathfrak{b} 
\sum_{j} \tau_{j} \frac{ \partial f_{i} }{ \partial x_{j} } 
\! \left( \mbox{\boldmath{$\sigma$}} \right)
+ \mathfrak{a} \sum_{j} \left( \upsilon_{j} + \mathfrak{b} \, \nu_{j} \right)
\left( \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mbox{\boldmath{$\sigma$}} \right)
+ \mathfrak{b} \sum_{k} \tau_{k}
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mbox{\boldmath{$\sigma$}} \right) \right) 
\\
%
=& \,
f_{i} \! \left( \mbox{\boldmath{$\sigma$}} \right) + \mathfrak{b} 
\sum_{j} \tau_{j} \frac{ \partial f_{i} }{ \partial x_{j} } 
\! \left( \mbox{\boldmath{$\sigma$}} \right)
+ \mathfrak{a} \sum_{j}
\upsilon_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mbox{\boldmath{$\sigma$}} \right)
\\
& + \mathfrak{a} \, \mathfrak{b} \sum_{j}
 \nu_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mbox{\boldmath{$\sigma$}} \right)
+ \mathfrak{a} \, \mathfrak{b} \, \sum_{jk} \upsilon_{j} \tau_{k}
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mbox{\boldmath{$\sigma$}} \right)
\\
%
=& \,
f_{i} \! \left( \mathbf{s} + \mathfrak{c} \, \delta \mathbf{s} \right) + \mathfrak{b} 
\sum_{j} \left( \delta t_{j} + \mathfrak{c} \, \delta^{2} t_{j} \right) 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} + \mathfrak{c} \, \delta \mathbf{s}\right)
\\
&+ 
\mathfrak{a} \sum_{j}
\left( \delta u_{j} + \mathfrak{c} \, \delta^{2} u_{j} \right) 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} + \mathfrak{c} \, \delta \mathbf{s}\right)
\\
& + \mathfrak{a} \, \mathfrak{b} \sum_{j}
\left( \delta^{2} v_{j} + \mathfrak{c} \, \delta^{3} v_{j} \right) 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} + \mathfrak{c} \, \delta \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \, \sum_{jk} 
\left( \delta u_{j} + \mathfrak{c} \, \delta^{2} u_{j} \right) 
\left( \delta t_{k} + \mathfrak{c} \, \delta^{2} t_{k} \right)
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} + \mathfrak{c} \, \delta \mathbf{s} \right)
\\
%
=& \,
f_{i} \! \left( \mathbf{s} \right) + \mathfrak{c} \sum_{j} \delta s_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{b}
\sum_{j} \left( \delta t_{j} + \mathfrak{c} \, \delta^{2} t_{j} \right) 
\left( \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+  \mathfrak{c} \sum_{k} \delta s_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) \right)
\\
&+ 
\mathfrak{a} \sum_{j}
\left( \delta u_{j} + \mathfrak{c} \, \delta^{2} u_{j} \right) 
\left( \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{c} \sum_{k} \delta s_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) \right)
\\
& + \mathfrak{a} \, \mathfrak{b} \sum_{j}
\left( \delta^{2} v_{j} + \mathfrak{c} \, \delta^{3} v_{j} \right) 
\left( \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{c} \sum_{k} \delta s_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \, \sum_{jk} 
\left( \delta u_{j} + \mathfrak{c} \, \delta^{2} u_{j} \right) 
\left( \delta t_{k} + \mathfrak{c} \, \delta^{2} t_{k} \right)
\\
& \quad\quad\quad\quad \times \left(
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) 
+ \mathfrak{c} \sum_{l} \delta s_{l} 
\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( \mathbf{s} \right)
\right)
\\
%
\end{align*}

\begin{align*}
f_{i} \! \left( \mbox{\boldmath{$\zeta$}} \right)
%
=& \,
f_{i} \! \left( s_{j} \right) + \mathfrak{c} \sum_{j} \delta s_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{b} \sum_{j} \delta t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{2} t_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{b} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta t_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \sum_{j} \delta u_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{a} \, \mathfrak{c} \sum_{j} \delta^{2} u_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{a} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta u_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \sum_{j} \delta^{2} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{3} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta^{2} v_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \, \sum_{jk} 
\left( \delta u_{j} \delta t_{k}
+ \mathfrak{c} \left(
\delta u_{j} \, \delta^{2} t_{k} + \delta^{2} u_{j} \, \delta t_{k} 
\right) \right)
\\
& \quad\quad\quad\quad \times \left(
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) 
+ \mathfrak{c} \sum_{l} \delta s_{l} 
\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( \mathbf{s} \right)
\right)
\\
%
=& \,
f_{i} \! \left( s_{j} \right) + \mathfrak{c} \sum_{j} \delta s_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{b} \sum_{j} \delta t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{2} t_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{b} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta t_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \sum_{j} \delta u_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{a} \, \mathfrak{c} \sum_{j} \delta^{2} u_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{a} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta u_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \sum_{j} \delta^{2} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{3} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta^{2} v_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \, \sum_{jk} 
\delta u_{j} \delta t_{k} \frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jk}
\left( \delta u_{j} \, \delta^{2} t_{k} + \delta^{2} u_{j} \, \delta t_{k}  \right)
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) 
\\
&+
\mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jkl}
\delta u_{j} \delta t_{k} \, \delta s_{l} 
\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( \mathbf{s} \right)
\\
%
\end{align*}

\begin{align*}
f_{i} \! \left( \mbox{\boldmath{$\zeta$}} \right)
%
=& \,
f_{i} \! \left( s_{j} \right) + \mathfrak{c} \sum_{j} \delta s_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{b} \sum_{j} \delta t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{2} t_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{b} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta t_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \sum_{j} \delta u_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{a} \, \mathfrak{c} \sum_{j} \delta^{2} u_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{a} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta u_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} 
\left( 
\sum_{j} \delta^{2} v_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
\delta u_{j} \delta t_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\right)
\\
&+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{3} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
\\
&+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jk}
\left( 
\delta s_{k} \, \delta^{2} v_{j} + \delta u_{j} \, \delta^{2} t_{k} + \delta^{2} u_{j} \, \delta t_{k}  \right)
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) 
\\
&+
\mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jkl}
\delta u_{j} \delta t_{k} \, \delta s_{l} 
\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( \mathbf{s} \right)
\\
%
=& \hspace{26mm} 
f_{i} \! \left( \mathbf{s} \right) 
\hspace{9mm}
+ \mathfrak{c} 
\hspace{3mm}
\sum_{j} \delta s_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
\\
& \hspace{8mm} + 
\mathfrak{b} \left( \sum_{j} \delta t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
\hspace{1mm}
+ \mathfrak{c} \left( 
\sum_{j} \delta^{2} t_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \sum_{jk} \delta s_{k} \, \delta t_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) \right) \right)
\\
&+ 
\mathfrak{a} \Bigg( 
\hspace{9mm}
\sum_{j} \delta u_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{c} \left( \sum_{j} \delta^{2} u_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \sum_{jk} \delta s_{k} \, \delta u_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) \right)
\\
& 
\hspace{8mm} 
+ \mathfrak{b} \Bigg(
\hspace{0.5mm}
\sum_{j} \delta^{2} v_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
\delta u_{j} \delta t_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
& \hspace{34mm} 
+ \mathfrak{c} \Bigg(
\sum_{j} \delta^{3} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
\\
& \hspace{40mm} 
+ \sum_{jk}
\left( 
\delta s_{k} \, \delta^{2} v_{j} + \delta u_{j} \, \delta^{2} t_{k} + \delta^{2} u_{j} \, \delta t_{k}  \right)
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) 
\\
& \hspace{40mm} +
\sum_{jkl}
\delta u_{j} \delta t_{k} \, \delta s_{l} 
\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( \mathbf{s} \right) 
\Bigg)
\Bigg)
\Bigg)
\\
%
\end{align*}
%
These results are summarized in Table \ref{tab:thirdOrder}.

\begin{table*}[t!]
	\centering
	\renewcommand{\arraystretch}{2}
	\begin{tabular}{ccc}
	\rowcolor[gray]{0.9} \textbf{Component} & \textbf{Input} & \textbf{Output} \\
	First Value & 
	$\mathbf{s}$ & 
	$f_{i} \! \left( \mathbf{s} \right)$ 
	\\
	\rowcolor[gray]{0.9} 
	First Gradient & 
	$\delta \mathbf{s} $ &
	$\displaystyle \sum_{j} \delta s_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) $
	\\
	Second Value & 
	$\delta \mathbf{t}$ & 
	$\displaystyle \sum_{j} \delta t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)$
	\\
	\rowcolor[gray]{0.9} 
	Second Gradient & 
	$\delta^{2} \mathbf{t}$ &
	$\displaystyle \sum_{j} \delta^{2} t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
	+ \sum_{jk} \delta s_{k} \, \delta t_{j} 
	\frac{ \partial f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)$
	\\
	Third Value & 
	$\delta \mathbf{u}$ &
	$\displaystyle \sum_{j} \delta u_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)$
	\\
	\rowcolor[gray]{0.9} 
	Third Gradient & 
	$\delta^{2} \mathbf{u}$ &
	$\displaystyle \sum_{j} \delta^{2} u_{j} 
	\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
	+ \sum_{jk} \delta s_{k} \, \delta u_{j} 
	\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)$
	\\
	Fourth Value & 
	$\delta^{2} \mathbf{v}$ & 
	$\displaystyle \sum_{j} \delta^{2} v_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
	\delta u_{j} \delta t_{k} 
	\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)$
	\\
	\rowcolor[gray]{0.9} 
   \multirow{3}{*}{ \vspace{-8mm} Fourth Gradient} & 
	\multirow{3}{*}{ \vspace{-8mm} $\delta^{3} \mathbf{v}$} &
	$\displaystyle \sum_{j} \delta^{3} v_{j} 
	\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) $ 
	\\
	& &
	$\displaystyle + \sum_{jk} \left( 
	\delta s_{k} \, \delta^{2} v_{j} + \delta u_{j} \, \delta^{2} t_{k} + \delta^{2} u_{j} \, \delta t_{k}  \right)
	\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) $ 
	\\
	\rowcolor[gray]{0.9} 
	& &
	$ \displaystyle+ \sum_{jkl} \delta u_{j} \delta t_{k} \, \delta s_{l} 
	\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( \mathbf{s} \right) $
	\\
	\end{tabular}
	\caption{Recursively expanding a function with respect to a third-order dual number 
	gives each component of the function's third-order pushforward.
	\label{tab:thirdOrder}}
\end{table*}

As with second-order pullbacks, third-order pullbacks require considering each values as 
a tangent and each gradient as a sensitivity.  For example, with the notation
%
\begin{align*}
s &\rightarrow x \\
\delta s &\rightarrow \mathrm{d} f \\
\delta t &\rightarrow \delta x \\
\delta^{2} t &\rightarrow \mathrm{d}^{2} f \\
\delta u &\rightarrow \delta x^{2} \\
\delta^{2} u &\rightarrow \mathrm{d}^{3} f \\
\delta^{2} v &\rightarrow \delta x^{3} \\
\delta^{3} v &\rightarrow \mathrm{d}^{4} f
\end{align*}
%
the third-order adjoint map is given by
%
\begin{align*}
\mathrm{d}^{4} x_{n}
&= \quad
\sum_{m = 1}^{M} \mathrm{d}^{4} f_{m} \frac{ \partial f_{m} }{ \partial x_{n} }
\\
& \quad + 
\sum_{m = 1}^{M} \mathrm{d} f_{m}
\left[ \sum_{n'=1}^{N} \delta^{3} x_{n'} \frac{ \partial^{2} f_{m} }{ \partial x_{n'} \partial x_{n} } \right]
+
\sum_{m = 1}^{M} \mathrm{d}^{2} f_{m}
\left[ \sum_{n'=1}^{N} \delta^{2} x_{n'} \frac{ \partial^{2} f_{m} }{ \partial x_{n'} \partial x_{n} } \right]
+
\sum_{m = 1}^{M} \mathrm{d}^{3} f_{m}
\left[ \sum_{n'=1}^{N} \delta x_{n'} \frac{ \partial^{2} f_{m} }{ \partial x_{n'} \partial x_{n} } \right]
\\
& \quad +
\sum_{m = 1}^{M} \mathrm{d} f_{m}
\left[ \sum_{n'=1}^{N} \sum_{n''=1}^{N} \delta x_{n'} \delta^{2} x_{n''} 
\frac{ \partial^{3} f_{m} }{ \partial x_{n''} \partial x_{n'} \partial x_{n} } \right]
\end{align*}

\section{Automatic Differentiation}

Automatic differentiation is the automated computation of linear differential operators applied
to a user-defined function.  Because dual numbers integrate a function evaluation along with
its pushforward or pullback, they provide a natural type -- ultimately, automatic differentiation 
is just the extension of a user-defined, real-valued function to accept dual numbers.

When these functions are composite, as is the case in almost any practical application, they
can be represented by an \textit{expression graph}, with each node carrying the value of
the intermediate function evaluation and each edge denoting a functional dependency
(Figure \ref{fig:exprGraph}).  The dual number-valued functions necessary for automatic
differentiation simply replace the single value at each node with the components of a
$k$th-order dual number.

\begin{figure}
\setlength{\unitlength}{0.1in} 
\centering
\begin{picture}(50, 20)
%
%\put(0, 0) { \framebox(50, 20){} }
%\put(25, 0) { \framebox(25, 30){} }
%\put(25, 0) { \framebox(6.25, 30){} }
%\put(25, 0) { \framebox(12.5, 30){} }
%\put(25, 0) { \framebox(18.75, 30){} }
%
%\put(25, 0) { \framebox(3.125, 30){} }
%\put(25, 0) { \framebox(9.375, 30){} }
%\put(25, 0) { \framebox(15.625, 30){} }
%
\put(12.5, 10) { \makebox(0, 0) 
{$g \! \left( f_{1} \! \left( x_{1}, x_{2} \right), f_{2} \! \left( x_{2}, x_{3} \right) \right)$} }
%
\put(21.875, 10) { \vector(1, 0){6.25} }
%
\put(31.25, 2.5) { \circle{4} }
\put(31.25, 2.5) { \makebox(0, 0) {$ x_{1} $} }
%
\put(37.5, 2.5) { \circle{4} }
\put(37.5, 2.5) { \makebox(0, 0) { $ x_{2} $ } }
%
\put(43.75, 2.5) { \circle{4} }
\put(43.75, 2.5) { \makebox(0, 0) { $ x_{3} $ } }
%
\put(31.25, 4.5) { \vector(3, 4){2.75} }
\put(37.5, 4.5) { \vector(-3, 4){2.75} }
\put(37.5, 4.5) { \vector(3, 4){2.75} }
\put(43.75, 4.5) { \vector(-3, 4){2.75} }
%
\put(35, 10) {\circle{4} } % Tweaked to the right
\put(34.375, 10) { \makebox(0, 0) { $f_{1}$ } }
%
\put(41.25, 10) {\circle{4} } % Tweaked to the right
\put(40.625, 10) { \makebox(0, 0) { $f_{2}$ } }
%
\put(34.375, 12) { \vector(3, 4){2.75} }
\put(40.625, 12) { \vector(-3, 4){2.75} }
%
\put(38, 17.5) {\circle{4} } % Tweaked to the right
\put(37.5, 17.5) { \makebox(0, 0) { $ g $ } }
%
\end{picture} 
\caption{
Composite functions are isomorphic to directed acyclic graphs known as 
\textit{expression graphs}; for example, the function
$g \! \left( f_{1} \! \left( x_{1}, x_{2} \right), f_{2} \! \left( x_{2}, x_{3} \right) \right)$
generates a three level graph, with leaves $\left\{ x_{1}, x_{2}, x_{3} \right\}$ and
root $g$.  The expression graph corresponding to a real-valued function
stores single values at each node, and automatic differentiation requires that
each node store all components of a $k$th-order dual number.
}
\label{fig:exprGraph} 
\end{figure}

The computation of any linear differential operator comprised entirely of pushforwards is
known as \textit{forward mode} or \textit{forward accumulation} automatic differentiation
because all dual numbers are propagated forward along the expression graph.  Any
adjoint operator, however, requires propagating sensitivities backwards along the expression
graph and consequently the implementation of adjoint operators is known as 
\textit{reverse mode} or \textit{reverse accumulation} automatic differentiation.  In this
section we consider both forward and reverse mode automatic differentiation, discuss
their relative performance, and demonstrate a selection of linear differential operator
implementations.

\subsection{Forward Mode Automatic Differentiation}

Forward mode automatic differentiation computes the $k$th-order pushforward 
of a given function by first initializing all values and gradients at the function
inputs and propagating the perturbations along the expression graph in a single 
forward sweep.  

Because only one sweep is necessary for forward mode, the expression graph does 
not need to be explicitly stored in memory, reducing overhead and often admitting 
more optimized code.  On the other hand, the linear differential operators comprised 
entirely of pushforwards are often not of immediate practical interest.

\subsection{Reverse Mode Automatic Differentiation}

Reverse mode automatic differentiation compliments forward mode by computing
$k$th-order adjoint operators which require more elaborate traversals of the
expression graph.  A general reverse mode calculation interleaves the forward
propagation of tangents and the reverse propagation of sensitivities, starting at
first order and working up to the desired order.

Consequently, reverse mode requires the persistent storage of the expression 
graph and induces a corresponding performance overhead.  In practice, however,
the adjoint operators admitted by reverse mode automatic differentiation are
more appropriate to applications and worth the additional cost.

\subsection{Relative Performance}

In general neither forward mode nor reverse mode automatic differentiation can be
claimed as optimal, as the relative performance of the two depends not only on
the given function but also on the desired linear differential operation.  

Consider, for example, many-to-one functions $f : \mathbb{R}^{N} \rightarrow \mathbb{R}$,
common to many applications.  Forward mode automatic differentiation, computes the 
directional derivative of the function along a given tangent in a single forward sweep.
Recovering the full gradient then requires $N$ forward sweeps, each initialized with
only one of the input tangents non-zero.  On the other hand, reverse mode automatic
differentiation computes the gradient with only one forward and one reverse sweep.  

If the directional derivative is sufficient or $N$ is small then forward mode automatic
differentiation is the better performing approach, but when the gradient is desired and 
$N$ is large, reverse mode automatic differentiation becomes far more computationally 
efficient despite the memory overhead.  Because this latter case is ubiquitous is
practice, \nomad implements reverse mode automatic differentiation.

\subsection{Example Implementations}

Here we review the exact computations needed to implement a selection of
linear differential operators natural to many-to-one functions, 
$f : \mathbb{R}^{N} \rightarrow 1$.

The gradient and Hessian are denoted by $\mathbf{g}$ and $\mathbf{H}$, respectively,
while adjuncts refers to lower-order linear differential operators that are generated
as a side effect of the desired operation.

\begin{tcolorbox}[colback=white,colframe=gray90, coltitle=black,boxrule=3pt,
fonttitle=\bfseries,title=Directional Derivative]

	\begin{description}
		\item[Form:] $\displaystyle \mathbf{v}^{T} \mathbf{g} = \sum_{i} v_{i} \frac{ \partial f }{ \partial x_{i} } $
		\item[Algorithm:] \hfill \\
		Initialize first gradient of the inputs, $\delta x_{i} = v_{i}$. \\
		Compute first-order forward sweep. \\
		Return first gradient of output.
		\item[Cost:] $\mathcal{O} \! \left( 1 \right)$
		\item[Adjuncts:] $ f $
	\end{description}
	
\end{tcolorbox}

\begin{tcolorbox}[colback=white,colframe=gray90, coltitle=black,boxrule=3pt,
fonttitle=\bfseries,title=Gradient]
	
	\begin{description}
		\item[Form:] $\displaystyle g_{i} = \frac{ \partial f }{ \partial x_{i} } $
		\item[Algorithm:] \hfill \\
		Initialize first gradient of the output, $\mathrm{d} f = 1$. \\
		Compute first-order reverse sweep. \\
		Return first gradient of inputs.
		\item[Cost:] $\mathcal{O} \! \left( 1 \right)$
		\item[Adjuncts:] $ f $
	\end{description}
	
\end{tcolorbox}

\begin{tcolorbox}[colback=white,colframe=gray90, coltitle=black,boxrule=3pt,
fonttitle=\bfseries,title=Hessian Quadratic Form]

	\begin{description}
		\item[Form:] 
		$\displaystyle \mathbf{v}^{T} \mathbf{H} \, \mathbf{u} = 
		\sum_{ij} v_{i} u_{j} \frac{ \partial^{2} f }{ \partial x_{i} \partial x_{j} } $
		\item[Algorithm:] \hfill \\
		Initialize first gradient, $\delta x_{i} = v_{i}$, second value, $\delta y_{i} = u_{i}$,
		and second gradient, $\delta^{2} y_{i} = 0$, of the inputs. \\
		Compute second-order forward sweep. \\
		Return second gradient of output.
		\item[Cost:] $\mathcal{O} \! \left( 1 \right)$
		\item[Adjuncts:] $ f, \, \mathbf{v}^{T} \mathbf{g}, \, \mathbf{u}^{T} \mathbf{g}$
	\end{description}
	
\end{tcolorbox}

\begin{tcolorbox}[colback=white,colframe=gray90, coltitle=black,boxrule=3pt,
fonttitle=\bfseries,title=Hessian-Vector Product]
	
	\begin{description}
		\item[Form:] $\displaystyle \mathbf{H} \, \mathbf{v} = 
		\sum_{j} v_{j} \frac{ \partial^{2} f }{ \partial x_{i} \partial x_{j} } $
		\item[Algorithm:] \hfill \\
		Initialize first gradient of the output, $\mathrm{d} f = 1$. \\
		Compute first-order reverse sweep for first gradients at the inputs. \\
		Initialize second values of the inputs, $\delta y_{i} = v_{i}$. \\
		Propagate second values in a forward sweep. \\
		Initialize second gradient of the output, $\mathrm{d}^{2} f = 0.$ \\
		Compute second-order reverse sweep. \\
		Return second gradient of inputs.
		\item[Cost:] $\mathcal{O} \! \left( 1 \right)$
		\item[Adjuncts:] $ f, \, \mathbf{v}^{T} \mathbf{g}, \, \mathbf{g}$
	\end{description}
	
\end{tcolorbox}

\begin{tcolorbox}[colback=white,colframe=gray90, coltitle=black,boxrule=3pt,
fonttitle=\bfseries,title=Hessian]
	
	\begin{description}
		\item[Form:] $\displaystyle H_{ij} = \frac{ \partial^{2} f }{ \partial x_{i} \partial x_{j} } $
		\item[Algorithm:] \hfill \\
		Initialize $\mathrm{d} f = 1$. \\
		Compute first-order reverse sweep for first gradients. \\
		For each $j$ compute the $j$th row of the Hessian as:
		\begin{itemize}
			\setlength{\itemsep}{0cm}
			\setlength{\parskip}{0cm}
			\item[] Initialize second values of the inputs, $\delta y_{i} = \delta^{j}_{i}$.
			\item[] Propagate second values in a forward sweep. 
			\item[] Initialize second gradient of the output, $\mathrm{d}^{2} f = 0$.
			\item[] Compute second-order reverse sweep.
			\item[] Return second gradient of inputs
		\end{itemize}
		\item[Cost:] $\mathcal{O} \! \left( n \right)$
		\item[Adjuncts:] $ f, \, \mathbf{g}$
	\end{description}
	
\end{tcolorbox}

\begin{tcolorbox}[colback=white,colframe=gray90, coltitle=black,boxrule=3pt,
fonttitle=\bfseries,title=Gradient of the Trace of a Matrix Hessian Product]
	
	\begin{description}
		\item[Form:] 
		$\displaystyle \frac{\partial}{\partial x_{i} } \mathrm{Tr} \! \left[ \mathbf{M} \, \mathbf{H} \right]
		= \sum_{jk} M_{jk} \frac{ \partial^{3} f }{ \partial x_{i}  \partial x_{j}  \partial x_{k} } $
		\item[Algorithm:] \hfill \\
		Initialize $\mathrm{d} f = 1$. \\
		Compute first-order reverse sweep for first gradients. \\
		Initialize the trace gradient to zero.
		For each $j$ increment the trace gradient with:
		\begin{itemize}
			\setlength{\itemsep}{0cm}
			\setlength{\parskip}{0cm}
			\item[] Initialize second values of the inputs, $\delta t_{i} = \delta^{j}_{i}$.
			\item[] Propagate second values in a forward sweep. 
			\item[] Initialize second gradient of the output, $\mathrm{d}^{2} f = 0$.
			\item[] Compute second-order reverse sweep.
			\item[] Initialize third and fourth values of the inputs, $\delta u_{i} = M_{ji}, \delta^{2} v_{i} = 0$.
			\item[] Propagate third and fourth values in a forward sweep.
			\item[] Initialize third gradient of the output, $\mathrm{d}^{3} f = 0$.
			\item[] Compute third-order reverse sweep.
			\item[] Return the fourth gradient of the inputs.
		\end{itemize}
		\item[Cost:] $\mathcal{O} \! \left( n \right)$
		\item[Adjuncts:] $ f, \, \mathbf{g}, \, \mathbf{H}$
	\end{description}
	
\end{tcolorbox}

