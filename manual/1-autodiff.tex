\chapter{Dual Numbers and the Theory of Automatic Differentiation}

Automatic differentiation is a tool for automating the chain rule when computing
derivatives of complex functions.  In particular, automatic differentiation techniques
utilize \textit{dual numbers} to factor analytic manipulations, i.e. the computation of
partial derivatives, from the algebraic manipulations that can be programmatically
automated.

In this note we review the theory behind dual numbers and automatic differentiation
and then explicitly derive the manipulations required for first, second, and third-order
derivative calculations.

\section{Notation}

Here frakturs refers to dual units,  roman letters are reserved for real numbers, and greek 
letters are used for dual numbers.  Subscripts denote components while bold face denote 
collections; for example, a many-to-one function,
%
\begin{equation*}
f : \mathbb{R}^{n} \rightarrow \mathbb{R},
\end{equation*}
%
is written as
%
\begin{equation*}
f \! \left( x_{1}, \ldots, x_{n} \right) \equiv f \! \left( \mathbf{x} \right),
\end{equation*} 
%
while the components of a many-to-many function,
%
\begin{equation*}
f : \mathbb{R}^{n} \rightarrow \mathbb{R}^{m},
\end{equation*}
%
are written as
%
\begin{equation*}
f_{j} \! \left( x_{1}, \ldots, x_{n} \right) \equiv f_{j} \! \left( \mathbf{x} \right), \, j \in 1, \ldots, m.
\end{equation*} 

\section{Dual Numbers}

Dual numbers are an extension of the real numbers, $\mathbb{R}$, given by the
addition of a new element, $\mathfrak{a}$, which is nilpotent, $\mathfrak{a}^{2} = 0$,
and denoted a \textit{dual unit}.

This extended space of dual numbers forms a two-dimensional associative algebra 
over the reals.  In other words, the addition of the dual unit generates a two-dimensional
space, $\mathbb{D}$, comprised of elements,
%
\begin{equation*}
\xi = x + \mathfrak{a} \, \delta x; \, \xi \in \mathbb{D}, \, x, \delta x \in \mathbb{R},
\end{equation*}
%
that are naturally equipped with addition and multiplication operations,
%
\begin{align*}
\xi_{1} + \xi_{2} 
&=
 \left( x_{1} + \mathfrak{a} \, \delta x_{1} \right) 
 + \left( x_{2} + \mathfrak{a} \,  \delta x_{2} \right) 
\\
&=
\left( x_{1} + x_{2} \right) 
+ \mathfrak{a} \left( \delta x_{1} + \delta x_{2} \right) 
\\
\xi_{1} \cdot \xi_{2} 
&= 
\left( x_{1} + \mathfrak{a} \, \delta x_{1} \right) 
\cdot \left( x_{2} + \mathfrak{a} \, \delta x_{2} \right) 
\\
&= 
x_{1} x_{2} + \left( x_{1} \delta x_{2} 
+ \delta x_{1} x_{2} \right) \mathfrak{a} 
+ \delta x_{1} \delta x_{2} \, \mathfrak{a}^{2} 
\\
&= 
x_{1} x_{2} + \left( x_{1} \delta x_{2} + \delta x_{1} x_{2} \right) \mathfrak{a}.
\end{align*}  
%
Because the dual unit is nilpotent, we cannot define an inverse to multiplication.  
Mathematically this means that the dual numbers form only a local ring; compare this to 
the complex numbers on which we can define division and whose associative algebra is a field.

Dual numbers have many uses in mathematics and physics, but here we will focus on their
relationship to functions.  Any smooth function $f : \mathbb{R} \rightarrow \mathbb{R}$ 
induces a function on the dual numbers, $f : \mathbb{D} \rightarrow \mathbb{D}$, 
via a Taylor series around any purely real point, $\xi_{0} = x_{0} + \mathfrak{a} \, 0$,
%
\begin{align*}
f \! \left( \xi \right) 
&= 
\sum_{n = 0}^{\infty} \frac{ \left( \xi - \xi_{0} \right)^{n} }{n!} 
\frac{ \partial^{n} f }{ \partial \xi^{n} } \! \left( \xi_{0} \right) 
\\
&= 
\sum_{n = 0}^{\infty} \frac{ \left( \left( x - x_{0} \right) + \mathfrak{a} \, \delta x \right)^{n} }{n!} 
\frac{ \partial^{n} f }{ \partial x^{n} } \! \left( x_{0} \right)
\\
&= 
\sum_{n = 0}^{\infty} \frac{ \left( x - x_{0} \right)^{n} }{n!} 
\frac{ \partial^{n} f }{ \partial x^{n} } \! \left( x_{0} \right)
+ \mathfrak{a} \, \delta x \sum_{n = 0}^{\infty} \frac{ \left( x - x_{0} \right)^{n - 1} }{\left( n - 1 \right)!} 
\frac{ \partial^{n} f }{ \partial x^{n} } \! \left( x_{0} \right) 
\\
&= 
f \! \left( x \right) 
+ \mathfrak{a} \, \delta x \frac{ \partial f }{ \partial x } \! \left( x \right).
\end{align*}
%
Evaluating a function at a dual number yields not just the value of the function but
also its best linear approximation; the dual unit can be thought of as formalizing the 
intuitive concept of an ``infinitesimal''.

The generalization to multivariate functions is straightforward: any function 
$f : \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ is first decomposed into $m$
functions, $f_{i} : \mathbb{R}^{n} \rightarrow \mathbb{R}$, which generalize
to dual numbers as
%
\begin{equation} \label{dualFunction}
f_{i} \! \left( \mbox{\boldmath{$\xi$}} \right) = f_{i} \! \left( \mathbf{x} \right) 
+ \mathfrak{a} \sum_{j = 1}^{n} 
\delta x_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x} \right).
\end{equation}
%
As in the univariate case, the dual component is given by the best linear approximation 
of the function, or its \textit{directional derivative along $\delta \mathbf{x}$}.  This is a
linear process, mapping the input vector $\delta \mathbf{x}$ through the Jacobian, 
$J : \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$, a linear map with components
$J_{ij} = \partial f_{i} / \partial x_{j}$.

This pattern continues to the composition of functions.  For example, if 
$f : \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ and 
$g : \mathbb{R}^{m} \rightarrow \mathbb{R}^{p}$, then the composition $g \circ f$
is given by the components,
%
\begin{align*}
g_{i} \! \left( \mathbf{f} \! \left( \mbox{\boldmath{$\xi$}} \right) \right)
&= 
g_{i} \! \left( \mathbf{f} \! \left( \mathbf{x} + \mathfrak{a} \, \delta \mathbf{x} \right) \right)
\\
&=
g_{i} \! \left( \mathbf{f} \! \left( \mathbf{x} \right) 
+ \mathfrak{a} \, \sum_{k} \delta x_{k} 
\frac{ \partial \mspace{1mu} \mathbf{f} }{ \partial x_{k} } \! \left( \mathbf{x} \right) \right)
\\
&=
g_{i} \! \left( \mathbf{f} \! \left( \mathbf{x} \right) \right)
+ \mathfrak{a} \, \sum_{k} \delta x_{k} 
\frac{ \partial g_{i} }{ \partial f_{j} }  \! \left( \mathbf{f} \! \left( \mathbf{x} \right) \right)
\frac{ \partial f_{j} }{ \partial x_{k} } \! \left( \mathbf{x} \right).
\end{align*}
%
As expected, the dual component of the composition is the best linear approximation
of the composite function, mapping $\delta \mathbf{x}$ through the Jacobians of both
functions.  It is also, however, exactly the result of evaluating the chain rule through
the composition.  Any implementation of dual number-valued functions implicitly provides 
an automated computation of the chain rule!

\section{First-Order Automatic Differentiation}

Automatic differentiation implements dual number-valued functions and their 
composition in order to automate the evaluation of the chain rule.  Here we take
advantage of the fact that any function can be represented by an 
\textit{expression graph}, with nodes designating dual numbers and edges denote 
functional dependencies (Figure \ref{fig:exprGraph}), where the evaluation of the 
function reduces to the propagation of the dual numbers through the graph.

\begin{figure}
\setlength{\unitlength}{0.1in} 
\centering
\begin{picture}(50, 30)
%
%\put(0, 0) { \framebox(50, 30){} }
%\put(25, 0) { \framebox(25, 30){} }
%\put(25, 0) { \framebox(6.25, 30){} }
%\put(25, 0) { \framebox(12.5, 30){} }
%\put(25, 0) { \framebox(18.75, 30){} }
%
%\put(25, 0) { \framebox(3.125, 30){} }
%\put(25, 0) { \framebox(9.375, 30){} }
%\put(25, 0) { \framebox(15.625, 30){} }
%
\put(12.5, 15) { \makebox(0, 0) 
{$z \! \left( y_{1} \! \left( x_{1}, x_{2} \right), y_{2} \! \left( x_{2}, x_{3} \right) \right)$} }
%
\put(21.875, 15) { \vector(1, 0){6.25} }
%
\put(31.25, 7.5) { \circle{4} }
\put(31.25, 7.5) { \makebox(0, 0) {$ x_{1} $} }
%
\put(37.5, 7.5) { \circle{4} }
\put(37.5, 7.5) { \makebox(0, 0) { $ x_{2} $ } }
%
\put(43.75, 7.5) { \circle{4} }
\put(43.75, 7.5) { \makebox(0, 0) { $ x_{3} $ } }
%
\put(31.25, 9.5) { \vector(3, 4){2.75} }
\put(37.5, 9.5) { \vector(-3, 4){2.75} }
\put(37.5, 9.5) { \vector(3, 4){2.75} }
\put(43.75, 9.5) { \vector(-3, 4){2.75} }
%
\put(35, 15) {\circle{4} } % Tweaked to the right
\put(34.375, 15) { \makebox(0, 0) { $y_{1}$ } }
%
\put(41.25, 15) {\circle{4} } % Tweaked to the right
\put(40.625, 15) { \makebox(0, 0) { $y_{2}$ } }
%
\put(34.375, 17) { \vector(3, 4){2.75} }
\put(40.625, 17) { \vector(-3, 4){2.75} }
%
\put(38, 22.5) {\circle{4} } % Tweaked to the right
\put(37.5, 22.5) { \makebox(0, 0) { $ z $ } }
%
\end{picture} 
\caption{
Composite functions are isomorphic to directed acyclic graphs known as 
an \textit{expression graph} or \textit{expression tree}.  Here we the function
$z \! \left( y_{1} \! \left( x_{1}, x_{2} \right), y_{2} \! \left( x_{2}, x_{3} \right) \right)$
generates a three level graph.
}
\label{fig:exprGraph} 
\end{figure}

In general the directionality of the propagation is not constrained, but in practice we 
typically consider only propagating dual numbers \textit{forward} with the evaluation
or \textit{reverse} to the evaluation.

\subsection{Forward Mode}

In forward mode automatic differentiation, the components of $\delta x_{j}$ defined at 
the roots are propagated forward through the expression graph using the Jacobian map at each
node (Figure \ref{fig:directions}).  Following the literature we will refer to the components of the dual
numbers as \textit{values},
%
\begin{equation*}
\mathcal{V} \! \left( \xi \right) 
= \mathcal{V} \! \left( x + \mathfrak{a} \, \delta x \right) 
= x,
\end{equation*}
%
and \textit{tangents} or \textit{perturbations},
\begin{equation*}
\mathcal{T} \! \left( \xi \right) 
= \mathcal{T} \! \left( x + \mathfrak{a} \, \delta x \right) 
= \delta x,
\end{equation*}
%
respectively.

\begin{figure}
\setlength{\unitlength}{0.1in} 
\centering
\begin{picture}(50, 30)
%
%\put(0, 0) { \framebox(50, 30){} }
%\put(0, 0) { \framebox(12.5, 30){} }
%\put(0, 0) { \framebox(25, 30){} }
%\put(0, 0) { \framebox(37.5, 30){} }
%\put(0, 0) { \framebox(50, 10){} }
%\put(0, 0) { \framebox(50, 20){} }
%
% Forward Mode
%
\put(12.5, 5) { \makebox(0, 0) { Forward Mode } }
%
\put(13, 10) {\circle{4} } % Tweak to the right
\put(12.5, 10) { \makebox(0, 0) { $ \xi_{j} $ } }
%
\put(12, 12) { \vector(0, 1){6} }
\put(13, 12) { \vector(0, 1){6} }
%
\put(13, 20) {\circle{4} } % Tweak to the right
\put(12.5, 20) { \makebox(0, 0) { $ \xi_{i} $ } }
%
\put(6.75, 15) { \makebox(0, 0) 
{ $ \mathcal{V} \! \left( \xi_{j} \right) = x_{j} \! \left( x_{i} \right)$ } }
\put(16.5, 16) { \makebox(0, 0) 
{ $ \mathcal{T} \! \left( \xi_{i} \right) =  $} }
\put(20, 14) { \makebox(0, 0) 
{ $ \sum_{i} \frac{ \partial x_{i} }{ \partial x_{j} } \mathcal{T} \! \left( \xi_{j} \right)  $ } }
%
% Reverse Mode
%
\put(37.5, 5) { \makebox(0, 0) { Reverse Mode } }
%
\put(38, 10) {\circle{4} } % Tweak to the right
\put(37.5, 10) { \makebox(0, 0) { $ \xi_{j} $ } }
%
\put(37, 12) { \vector(0, 1){6} }
\put(38, 18) { \vector(0, -1){6} }
%
\put(38, 20) {\circle{4} } % Tweak to the right
\put(37.5, 20) { \makebox(0, 0) { $ \xi_{i} $ } }
%
\put(31.75, 15) { \makebox(0, 0) 
{ $ \mathcal{V} \! \left( x_{j} \right) = x_{j} \! \left( x_{i} \right)$ } }
\put(41.5, 14) { \makebox(0, 0) 
{ $ \mathcal{A} \! \left( \xi_{j} \right) =  $} }
\put(45, 16) { \makebox(0, 0) 
{ $ \sum_{i} \frac{ \partial x_{i} }{ \partial x_{j} } \mathcal{A} \! \left( \xi_{i} \right)  $ } }
%
\end{picture} 
\caption{
In forward mode automatic differentiation the Jacobian propagates directional 
derivatives, $\mathcal{T} \! \left( \xi \right)$ forward in the same direction as the
values, $\mathcal{V} \! \left( \xi \right)$.  Reverse mode automatic differentiation, 
on the other hand, uses the adjoint Jacobian to propagate adjoint directional 
derivatives, $\mathcal{A} \! \left( \xi \right)$, backwards against the 
values, $\mathcal{V} \! \left( \xi \right)$.
}
\label{fig:directions} 
\end{figure}

Messages accumulated at the output nodes yield the components of the directional
derivative, $\sum_{k} \delta x_{k} \partial f_{j} / \partial x_{k}$.

\subsection*{Reverse Mode}

In reverse mode autodiff we consider not the Jacobian but rather its \textit{adjoint},
$J^{T} : \mathbb{R}^{m} \rightarrow \mathbb{R}^{n}$, which maps the second
component of dual numbers at the outputs, denoted $\mathrm{d} f_{i}$ to differentiate
them from their use in forward mode autodiff, to the adjoint directional derivative, 
$\sum_{j} \mathrm{d} f_{j} \partial f_{j} / \partial x_{k}$.  Using the adjoint Jacobian
we can propagate the $\mathrm{d} f_{i}$ at the outputs backwards through the 
expression graph until the input nodes yield the components of the transposed
directional derivative (Figure \ref{fig:directions}).

When considering the action of the adjoint Jacobian the components of the dual
numbers as denoted \textit{values},
%
\begin{equation*}
\mathcal{V} \! \left( \xi \right) 
= \mathcal{V} \! \left( x + \mathfrak{a} \, \mathrm{d} x \right) 
= x,
\end{equation*}
%
and \textit{adjoints} or \textit{sensitivities},
\begin{equation*}
\mathcal{A} \! \left( \xi \right) 
= \mathcal{A} \! \left( x + \mathfrak{a} \, \mathrm{d} x \right) 
= \mathrm{d} x,
\end{equation*}
%
respectively.

\subsection*{Performance}

The relative performance of forward and reverse mode autodiff depends on
the sizes of $n$ and $m$ and the desired components of the Jacobian.  Note
that per evaluation forward mode will be faster than reverse mode as it requires
only one sweep compared to the two reverse mode requires (one forward sweep
to build up the expression graph and one reverse sweep to compute the adjoint
directional derivative).  Moreover the overhead for forward mode will be larger
since it does not require the full expression graph to be stored.

Consider, for example, the many-to-one case where $m = 1$.  Here forward
mode computes the scalar $\sum_{k} \delta x_{i} \partial f / \partial x_{i}$
where as the reverse mode computes the full vector gradient 
$\mathrm{d} f \partial f / \partial x_{i}$.  If the directional derivative is all
that is necessary then the forward mode calculation will be quicker given
the considerations above.  On the other hand, if the full vector gradient is required 
then the small overhead from reverse mode will be dwarfed by the $m$ repetitions 
required to build the full gradient from directional derivatives in forward mode.

In general, computing the full Jacobian is faster with forward mode if $n \ll m$ 
and faster with reverse mode if $n \gg m$.

\section*{Higher-Order Automatic Differentiation}

One substantial advantage of dual numbers is that they dramatically ease the
manipulation of higher-order variants of the chain rule.  In order to go to 
higher-order, however, we need to introduce nested dual numbers.

For example, let $\mathfrak{a}$ and $\mathfrak{b}$ be two distinct dual units.
A first-order dual number,
%
\begin{equation*}
\zeta_{i} = z_{i} + \mathfrak{a} \, \delta z_{i},
\end{equation*}
%
then becomes a second-order dual number by replacing the 
real-valued components with dual numbers in the direction of the second 
dual unit,
%
\begin{alignat*}{3}
\zeta_{i} 
&=
\xi_{i} 
&&+ \mathfrak{a} \, \eta_{i}
\\
&=
\left( x_{i} + \mathfrak{b} \, \delta x_{i} \right)
&&+ \mathfrak{a} \left( \delta y_{i} + \mathfrak{b} \, \delta^{2} y_{i} \right).
\end{alignat*}
%
I will refer to $x_{i}$ as the first value, $\delta x_{i}$ as the first gradient, 
$\delta y_{i}$ as the second value, and $\delta^{2} y_{i}$
as the second gradient.  Higher-order dual numbers follow recursively 
by replacing the real-valued components with additional dual numbers along 
distinct dual units.

Higher-order dual numbers are evaluated in the same manner of forward and 
reverse sweeps as the first order dual numbers, with the propagation rules
for each component generated by the dual algebra.  Forward and reverse
propagations yield various directional derivatives as defined in Table \ref{tab:directDerivs}.

\begin{table*}[t!]
	\centering
	\renewcommand{\arraystretch}{2}
	\begin{tabular}{cccc}
	\rowcolor[gray]{0.9} \textbf{Value Type} & \textbf{Order} 
	& \textbf{Formula} & \textbf{Mode} 
	\\
	Scalar & First & 
	$ \displaystyle \sum_{j} v_{j} \, f_{i} \! \left( x_{j} \right)$ & Forward
	\\
	\rowcolor[gray]{0.9}
	Vector & First & 
	$ \displaystyle  f_{i} \! \left( x_{j} \right)$ & Reverse
	\\
	Scalar & Second & 
	$ \displaystyle \sum_{jk} v_{j} \, u_{k} \frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} }$ 
	& Forward
	\\
	\rowcolor[gray]{0.9}
	Vector & Second & 
	$ \displaystyle \sum_{j} v_{j} \frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} }$  
	& Reverse
	\\
	Scalar & Third & 
	$ \displaystyle \sum_{jkl} v_{j} \, u_{k} \, w_{l} 
	\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l}}$  
	& Forward
	\\
	\rowcolor[gray]{0.9}
	Vector & Third & 
	$\displaystyle \sum_{jk} v_{j} \, u_{k} \frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l}}$
	& Reverse
	\\
	\end{tabular}
	\caption{Automatic differentiation computes directional derivatives and
	generalizations thereof.  In general, forward mode calculations return
	scalars while reverse mode calculations return vectors.
	\label{tab:directDerivs}}
\end{table*}

Here we compute the propagation rules for second and third-order directional 
derivatives to complement the first-order rules derived above before considering 
how to use these rules to compute popular differential objects such as the Hessian.

\subsection*{Second-Order}

The extension of a function $f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ to
a second-order dual number follows from the recursive application of the
first-order extension: given the dual number
%
\begin{alignat*}{3}
\zeta_{i} 
&=
\xi_{i} 
&&+ \mathfrak{a} \, \eta_{i}
\\
&=
\left( x_{i} + \mathfrak{b} \, \delta x_{i} \right)
&&+ \mathfrak{a} \left( \delta y_{i} + \mathfrak{b} \, \delta^{2} y_{i} \right)
\end{alignat*}
%
we have
%
\begin{align*}
f_{i} \! \left( \zeta_{j} \right)
=&
f_{i} \! \left( \xi_{j} \right) 
+ \mathfrak{a} \sum_{j} \eta_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \xi_{j} \right) 
\\
=&
f_{i} \! \left( x_{j} + \mathfrak{b} \, \delta x_{j} \right) 
+ \mathfrak{a} \sum_{j} \left( \delta y_{j} + \mathfrak{b} \, \delta^{2} y_{j} \right)
\frac{ \partial f_{i} }{ \partial x_{j} } 
\! \left( x_{j} + \mathfrak{b} \, \delta x_{j} \right) 
\\
=&
f_{i} \! \left( x_{j} \right) 
+ \mathfrak{b} \sum_{j} \delta x_{j}  \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( x_{j} \right)
\\
&+ 
\mathfrak{a} \sum_{j} \left( \delta y_{j} + \mathfrak{b} \, \delta^{2} y_{j} \right)
\left( \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( x_{j} \right) + 
\mathfrak{b} \, \sum_{k} \delta x_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( x_{j} \right) \right) 
\\
=&
\quad\quad\quad 
f_{i} \! \left( x_{j} \right) 
\quad\quad\quad\;\;\;
+ \mathfrak{b} \;\;\;
\sum_{j} \delta x_{j}  \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( x_{j} \right)
\\
&+ 
\mathfrak{a} \left( 
\sum_{j} \delta y_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( x_{j} \right) 
+ \mathfrak{b} \left(
\sum_{j} \delta^{2} y_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( x_{j} \right)
+ \sum_{jk} \delta x_{k} \, \delta y_{j}
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( x_{j} \right)
\right)
\right)
\end{align*}
%
These results are summarized in Table \ref{tab:secondOrder}.

\begin{table*}[t!]
	\centering
	\renewcommand{\arraystretch}{2}
	\begin{tabular}{ccc}
	\rowcolor[gray]{0.9} \textbf{Component} & \textbf{Input} & \textbf{Output} \\
	First Value & 
	$x_{i}$ & 
	$f_{i} \! \left( x_{j} \right)$ 
	\\
	\rowcolor[gray]{0.9} 
	First Gradient & 
	$\delta x_{i}$ &
	$\displaystyle \sum_{j} \delta x_{j}  \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( x_{j} \right)$
	\\
	Second Value & 
	$\delta y_{i}$ & 
	$\displaystyle \sum_{j} \delta y_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( x_{j} \right)$
	\\
	\rowcolor[gray]{0.9} 
	Second Gradient & 
	$\delta^{2} y_{i}$ & 
	$\displaystyle \sum_{j} \delta^{2} y_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( x_{j} \right)
	+ \sum_{jk} \delta x_{k} \, \delta y_{j}
	\frac{ \partial f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( x_{j} \right)$
	\\
	\end{tabular}
	\caption{Recursively expanding an input function yields its action
	on a second-order dual number input.
	\label{tab:secondOrder}}
\end{table*}

In forward mode we compute four different values at each node -- the function
evaluation, a directional derivative along $\delta x_{i}$, a directional derivative
along $\delta y_{i}$, and the scalar-valued second-order directional derivative
$ \sum_{j} \delta^{2} y_{j} \, \partial f_{i} / \partial x_{j}
+ \sum_{jk} \delta x_{k} \, \delta y_{j} \, \partial f_{i} / \partial x_{j} \partial x_{k}$.

The generalization of reverse mode requires some care because the second-order
Jacobian, $\partial f_{i} / \partial x_{j} \partial x_{k}$, does not have a well-defined
transpose.  If we propagate the second-order values forward first, however,
then we can define the linear operator 
$\delta y_{j} \, \partial f_{i} / \partial x_{j} \partial x_{k}$ which can be transposed.
Second-order reverse mode then consists of a forward sweep in which the
first and second-order values are computed, and then reverse sweep in which
the first and second-order gradients are computed.  This yields the function
evaluation, the full gradient, a directional derivative along $\delta y_{j}$, and
the vector-valued second-order directional derivative 
$ \sum_{j} \mathrm{d}^{2} y_{j} \, \partial f_{i} / \partial x_{j}
+ \sum_{jk} \mathrm{d} x_{k} \, \delta y_{j} \, \partial f_{i} / \partial x_{j} \partial x_{k}$

\subsection*{Third-Order}

Continuing to third-order follows in kind.  Given a third-order dual number,
%
\begin{alignat*}{10}
\zeta_{i} 
&=
&& \xi_{i} && && &&
&&+ \mathfrak{a} \, 
&& \eta_{i} && && &&
\\
&=
( && \sigma_{i} && + \mathfrak{b} && \tau_{i} &&)
&&+ \mathfrak{a} \,
( && \upsilon_{i} && + \mathfrak{b} && \nu_{i} && )
\\
&=
(( && s_{i} + \mathfrak{c} \, \delta s_{i} )
&& + \mathfrak{b} \;
( && \delta t_{i} + \mathfrak{c} \, \delta^{2} t_{i}  ) &&)
&&+ \mathfrak{a} \;
(( && \delta u_{i} + \mathfrak{c} \, \delta^{2} u_{i} )
&& + \mathfrak{b} \;
( && \delta^{2} v_{i} + \mathfrak{c} \, \delta^{3} v_{i} ) &&)
\end{alignat*}
%
we have
%
\begin{align*}
f_{i} \! \left( \zeta_{j} \right)
%
=&
f_{i} \! \left( \xi_{j} \right) 
+ \mathfrak{a} \sum_{j} \eta_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \xi_{j} \right) 
\\
%
=&
f_{i} \! \left( \sigma_{j} + \mathfrak{b} \, \tau_{j} \right) 
+ \mathfrak{a} \sum_{j} \left( \upsilon_{j} + \mathfrak{b} \, \nu_{j} \right)
\frac{ \partial f_{i} }{ \partial x_{j} } 
\! \left( \sigma_{j} + \mathfrak{b} \, \tau_{j} \right) 
\\
%
=&
f_{i} \! \left( \sigma_{j} \right) + \mathfrak{b} 
\sum_{j} \tau_{j} \frac{ \partial f_{i} }{ \partial x_{j} } 
\! \left( \sigma_{j} \right)
+ \mathfrak{a} \sum_{j} \left( \upsilon_{j} + \mathfrak{b} \, \nu_{j} \right)
\left( \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \sigma_{j} \right)
+ \mathfrak{b} \sum_{k} \tau_{k}
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \sigma_{j} \right) \right) 
\\
%
=&
f_{i} \! \left( \sigma_{j} \right) + \mathfrak{b} 
\sum_{j} \tau_{j} \frac{ \partial f_{i} }{ \partial x_{j} } 
\! \left( \sigma_{j} \right)
+ \mathfrak{a} \sum_{j}
\upsilon_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \sigma_{j} \right)
\\
& + \mathfrak{a} \, \mathfrak{b} \sum_{j}
 \nu_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \sigma_{j} \right)
+ \mathfrak{a} \, \mathfrak{b} \, \sum_{jk} \upsilon_{j} \tau_{k}
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \sigma_{j} \right)
\\
%
=&
f_{i} \! \left( s_{j} + \mathfrak{c} \, \delta s_{j} \right) + \mathfrak{b} 
\sum_{j} \left( \delta t_{j} + \mathfrak{c} \, \delta^{2} t_{j} \right) 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} + \mathfrak{c} \, \delta s_{j} \right)
\\
&+ 
\mathfrak{a} \sum_{j}
\left( \delta u_{j} + \mathfrak{c} \, \delta^{2} u_{j} \right) 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} + \mathfrak{c} \, \delta s_{j} \right)
\\
& + \mathfrak{a} \, \mathfrak{b} \sum_{j}
\left( \delta^{2} v_{j} + \mathfrak{c} \, \delta^{3} v_{j} \right) \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} + \mathfrak{c} \, \delta s_{j} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \, \sum_{jk} 
\left( \delta u_{j} + \mathfrak{c} \, \delta^{2} u_{j} \right) 
\left( \delta t_{k} + \mathfrak{c} \, \delta^{2} t_{k} \right)
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} + \mathfrak{c} \, \delta s_{j} \right)
\\
%
=&
f_{i} \! \left( s_{j} \right) + \mathfrak{c} \sum_{j} \delta s_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
\\
&+ 
\mathfrak{b}
\sum_{j} \left( \delta t_{j} + \mathfrak{c} \, \delta^{2} t_{j} \right) 
\left( \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
+  \mathfrak{c} \sum_{k} \delta s_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right) \right)
\\
&+ 
\mathfrak{a} \sum_{j}
\left( \delta u_{j} + \mathfrak{c} \, \delta^{2} u_{j} \right) 
\left( \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
+ \mathfrak{c} \sum_{k} \delta s_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right) \right)
\\
& + \mathfrak{a} \, \mathfrak{b} \sum_{j}
\left( \delta^{2} v_{j} + \mathfrak{c} \, \delta^{3} v_{j} \right) 
\left( \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
+ \mathfrak{c} \sum_{k} \delta s_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right) \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \, \sum_{jk} 
\left( \delta u_{j} + \mathfrak{c} \, \delta^{2} u_{j} \right) 
\left( \delta t_{k} + \mathfrak{c} \, \delta^{2} t_{k} \right)
\\
& \quad\quad\quad\quad \times \left(
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right) 
+ \mathfrak{c} \sum_{l} \delta s_{l} 
\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( s_{j} \right)
\right)
\\
%
\end{align*}

\begin{align*}
f_{i} \! \left( \zeta_{j} \right)
%
=&
f_{i} \! \left( s_{j} \right) + \mathfrak{c} \sum_{j} \delta s_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
\\
&+ 
\mathfrak{b} \sum_{j} \delta t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
+ \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{2} t_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
+ \mathfrak{b} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta t_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right)
\\
&+ 
\mathfrak{a} \sum_{j} \delta u_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
+ \mathfrak{a} \, \mathfrak{c} \sum_{j} \delta^{2} u_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
+ \mathfrak{a} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta u_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \sum_{j} \delta^{2} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{3} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta^{2} v_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \, \sum_{jk} 
\left( \delta u_{j} \delta t_{k}
+ \mathfrak{c} \left(
\delta u_{j} \, \delta^{2} t_{k} + \delta^{2} u_{j} \, \delta t_{k} 
\right) \right)
\\
& \quad\quad\quad\quad \times \left(
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right) 
+ \mathfrak{c} \sum_{l} \delta s_{l} 
\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( s_{j} \right)
\right)
\\
%
=&
f_{i} \! \left( s_{j} \right) + \mathfrak{c} \sum_{j} \delta s_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
\\
&+ 
\mathfrak{b} \sum_{j} \delta t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
+ \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{2} t_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
+ \mathfrak{b} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta t_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right)
\\
&+ 
\mathfrak{a} \sum_{j} \delta u_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
+ \mathfrak{a} \, \mathfrak{c} \sum_{j} \delta^{2} u_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
+ \mathfrak{a} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta u_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \sum_{j} \delta^{2} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{3} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta^{2} v_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \, \sum_{jk} 
\delta u_{j} \delta t_{k} \frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right)
\\
&+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jk}
\left( \delta u_{j} \, \delta^{2} t_{k} + \delta^{2} u_{j} \, \delta t_{k}  \right)
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right) 
\\
&+
\mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jkl}
\delta u_{j} \delta t_{k} \, \delta s_{l} 
\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( s_{j} \right)
\\
%
\end{align*}

\begin{align*}
f_{i} \! \left( \zeta_{j} \right)
%
=&
f_{i} \! \left( s_{j} \right) + \mathfrak{c} \sum_{j} \delta s_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
\\
&+ 
\mathfrak{b} \sum_{j} \delta t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
+ \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{2} t_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
+ \mathfrak{b} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta t_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right)
\\
&+ 
\mathfrak{a} \sum_{j} \delta u_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
+ \mathfrak{a} \, \mathfrak{c} \sum_{j} \delta^{2} u_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
+ \mathfrak{a} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta u_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} 
\left( 
\sum_{j} \delta^{2} v_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
\delta u_{j} \delta t_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right)
\right)
\\
&+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{3} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
\\
&+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jk}
\left( 
\delta s_{k} \, \delta^{2} v_{j} + \delta u_{j} \, \delta^{2} t_{k} + \delta^{2} u_{j} \, \delta t_{k}  \right)
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right) 
\\
&+
\mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jkl}
\delta u_{j} \delta t_{k} \, \delta s_{l} 
\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( s_{j} \right)
\\
%
=& \hspace{26mm} 
f_{i} \! \left( s_{j} \right) 
\hspace{9mm}
+ \mathfrak{c} 
\hspace{3mm}
\sum_{j} \delta s_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
\\
& \hspace{8mm} + 
\mathfrak{b} \left( \sum_{j} \delta t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
\hspace{1mm}
+ \mathfrak{c} \left( 
\sum_{j} \delta^{2} t_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
+ \sum_{jk} \delta s_{k} \, \delta t_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right) \right) \right)
\\
&+ 
\mathfrak{a} \Bigg( 
\hspace{9mm}
\sum_{j} \delta u_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
+ \mathfrak{c} \left( \sum_{j} \delta^{2} u_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
+ \sum_{jk} \delta s_{k} \, \delta u_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right) \right)
\\
& 
\hspace{8mm} 
+ \mathfrak{b} \Bigg(
\hspace{0.5mm}
\sum_{j} \delta^{2} v_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
\delta u_{j} \delta t_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right)
\\
& \hspace{34mm} 
+ \mathfrak{c} \Bigg(
\sum_{j} \delta^{3} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
\\
& \hspace{40mm} 
+ \sum_{jk}
\left( 
\delta s_{k} \, \delta^{2} v_{j} + \delta u_{j} \, \delta^{2} t_{k} + \delta^{2} u_{j} \, \delta t_{k}  \right)
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right) 
\\
& \hspace{40mm} +
\sum_{jkl}
\delta u_{j} \delta t_{k} \, \delta s_{l} 
\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( s_{j} \right) 
\Bigg)
\Bigg)
\Bigg)
\\
%
\end{align*}
%
These results are summarized in Table \ref{tab:thirdOrder}.

\begin{table*}[t!]
	\centering
	\renewcommand{\arraystretch}{2}
	\begin{tabular}{ccc}
	\rowcolor[gray]{0.9} \textbf{Component} & \textbf{Input} & \textbf{Output} \\
	First Value & 
	$s_{i}$ & 
	$f_{i} \! \left( s_{j} \right)$ 
	\\
	\rowcolor[gray]{0.9} 
	First Gradient & 
	$\delta s_{i}$ &
	$\displaystyle \sum_{j} \delta s_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) $
	\\
	Second Value & 
	$\delta t_{i}$ & 
	$\displaystyle \sum_{j} \delta t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)$
	\\
	\rowcolor[gray]{0.9} 
	Second Gradient & 
	$\delta^{2} t_{i}$ &
	$\displaystyle \sum_{j} \delta^{2} t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
	+ \sum_{jk} \delta s_{k} \, \delta t_{j} 
	\frac{ \partial f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right)$
	\\
	Third Value & 
	$\delta u_{i}$ &
	$\displaystyle \sum_{j} \delta u_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)$
	\\
	\rowcolor[gray]{0.9} 
	Third Gradient & 
	$\delta^{2} u_{i}$ &
	$\displaystyle \sum_{j} \delta^{2} u_{j} 
	\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) 
	+ \sum_{jk} \delta s_{k} \, \delta u_{j} 
	\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right)$
	\\
	Fourth Value & 
	$\delta^{2} v_{i}$ & 
	$\displaystyle \sum_{j} \delta^{2} v_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right)
	\delta u_{j} \delta t_{k} 
	\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right)$
	\\
	\rowcolor[gray]{0.9} 
       \multirow{3}{*}{ \vspace{-8mm} Fourth Gradient} & 
	\multirow{3}{*}{ \vspace{-8mm} $\delta^{3} v_{i}$} &
	$\displaystyle \sum_{j} \delta^{3} v_{j} 
	\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( s_{j} \right) $ 
	\\ 
	& &
	$\displaystyle + \sum_{jk} \left( 
	\delta s_{k} \, \delta^{2} v_{j} + \delta u_{j} \, \delta^{2} t_{k} + \delta^{2} u_{j} \, \delta t_{k}  \right)
	\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( s_{j} \right) $ 
	\\
	\rowcolor[gray]{0.9} 
	& &
	$ \displaystyle+ \sum_{jkl} \delta u_{j} \delta t_{k} \, \delta s_{l} 
	\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( s_{j} \right) $
	\\
	\end{tabular}
	\caption{Recursively expanding an input function yields its action
	on a third-order dual number input.
	\label{tab:thirdOrder}}
\end{table*}

A forward sweep of third-order autodiff yields a wealth of information.  In addition
to the function evaluation we have three different first-order scalar-valued directional 
derivatives, three different second-order scalar-valued directional derivatives,
and a single third-order scalar-valued directional derivative.

As above, reverse mode requires a separation of values and gradients.  A
preliminary forwards sweep first builds the expression graph and computes the
function, three first-order scalar-valued directional derivatives, and a second-order
scalar-valued directional derivative.  Given the values we can define adjoint
Jacobians and sweep backwards, generating the vector-valued gradient,
two second-order vector-valued directional derivatives, and one third-order
vector-valued directional derivative.

\subsection*{Higher-Order Techniques}

Lastly let's review some techniques for computing various differential objects that
are common in statistical and optimization applications.  Here we focus on many-
to-one functions, $f : \mathbb{R}^{n} \rightarrow 1$.

\begin{description}
	
	\item[Directional Derivative] \hfill \\
	
	\begin{description}
		\item[Form:] $\displaystyle \vec{v} \cdot \vec{g} = \sum_{i} v_{i} \frac{ \partial f }{ \partial x_{i} } $
		\item[Algorithm:] \hfill \\
		Initialize $\delta x_{i} = v_{i}$. \\
		Compute first-order forward sweep. \\
		Return first gradient of output.
		\item[Cost:] $\mathcal{O} \! \left( 1 \right)$
		\item[Adjuncts:] $ f $
	\end{description}

	\item[Gradient] \hfill \\
	\begin{description}
		\item[Form:] $\displaystyle \vec{g} = \frac{ \partial f }{ \partial x_{i} } $
		\item[Algorithm:] \hfill \\
		Initialize $\mathrm{d} f = 1$. \\
		Compute first-order reverse sweep. \\
		Return first gradient of inputs.
		\item[Cost:] $\mathcal{O} \! \left( 1 \right)$
		\item[Adjuncts:] $ f $
	\end{description}
	
	\item[Hessian Quadratic Form] \hfill \\
	\begin{description}
		\item[Form:] 
		$\displaystyle \vec{v\,}^{T} H \, \vec{u} = \sum_{ij} v_{i} u_{j} \frac{ \partial^{2} f }{ \partial x_{i} \partial x_{j} } $
		\item[Algorithm:] \hfill \\
		Initialize $\delta x_{i} = v_{i}, \, \delta y_{i} = u_{i}, \delta^{2} y_{i} = 0$. \\
		Compute second-order forward sweep. \\
		Return second gradient of output.
		\item[Cost:] $\mathcal{O} \! \left( 1 \right)$
		\item[Adjuncts:] $ f, \vec{v} \cdot \vec{g}, \vec{u} \cdot \vec{g}$
	\end{description}
	
	\item[Hessian-Vector Product] \hfill \\
	\begin{description}
		\item[Form:] $\displaystyle H \vec{v} = \sum_{j} v_{j} \frac{ \partial^{2} f }{ \partial x_{i} \partial x_{j} } $
		\item[Algorithm:] \hfill \\
		Initialize $\mathrm{d} f = 1$. \\
		Compute first-order reverse sweep for first gradients. \\
		Initialize $\delta y_{i} = v_{i}$. \\
		Propagate second values in a forward sweep. \\
		Initialize $\mathrm{d}^{2} f = 0.$ \\
		Compute second-order reverse sweep. \\
		Return second gradient of inputs.
		\item[Cost:] $\mathcal{O} \! \left( 1 \right)$
		\item[Adjuncts:] $ f, \vec{v} \cdot \vec{g}, \vec{g}$
	\end{description}
	
	\item[Hessian] \hfill \\
	\begin{description}
		\item[Form:] $\displaystyle \frac{ \partial^{2} f }{ \partial x_{i} \partial x_{j} } $
		\item[Algorithm:] \hfill \\
		Initialize $\mathrm{d} f = 1$. \\
		Compute first-order reverse sweep for first gradients. \\
		For each $j$ compute the $j$th row of the Hessian as:
		\begin{itemize}
			\setlength{\itemsep}{0cm}
			\setlength{\parskip}{0cm}
			\item[] Initialize $\delta y_{i} = \delta^{j}_{i}$.
			\item[] Propagate second values in a forward sweep. 
			\item[] Initialize $\mathrm{d}^{2} f = 0.$
			\item[] Compute second-order reverse sweep.
			\item[] Return second gradient of inputs
		\end{itemize}
		\item[Cost:] $\mathcal{O} \! \left( n \right)$
		\item[Adjuncts:] $ f, \vec{g}$
	\end{description}
	
	\item[Gradient of the Trace of a Matrix Hessian Product] \hfill \\
	\begin{description}
		\item[Form:] 
		$\displaystyle \frac{\partial}{\partial x_{i} } \mathrm{Tr} \! \left[ M \cdot H \right]
		= \sum_{jk} M_{jk} \frac{ \partial^{3} f }{ \partial x_{i}  \partial x_{j}  \partial x_{k} } $
		\item[Algorithm:] \hfill \\
		Initialize $\mathrm{d} f = 1$. \\
		Compute first-order reverse sweep for first gradients. \\
		Initialize the trace gradient to zero.
		For each $j$ increment the trace gradient with:
		\begin{itemize}
			\setlength{\itemsep}{0cm}
			\setlength{\parskip}{0cm}
			\item[] Initialize $\delta t_{i} = \delta^{j}_{i}$.
			\item[] Propagate second values in a forward sweep. 
			\item[] Initialize $\mathrm{d}^{2} f = 0$.
			\item[] Compute second-order reverse sweep.
			\item[] Initialize $\delta u_{i} = M_{ji}, \delta^{2} v_{i} = 0$.
			\item[] Propagate third and fourth values in a  forward sweep.
			\item[] Initialize $\mathrm{d}^{3} f = 0$.
			\item[] Compute third-order reverse sweep.
			\item[] Return the fourth gradient of the inputs.
		\end{itemize}
		\item[Cost:] $\mathcal{O} \! \left( n \right)$
		\item[Adjuncts:] $ f, \vec{g}, H$
	\end{description}
	
\end{description}
