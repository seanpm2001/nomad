\chapter{The Theory of \\ Automatic Differentiation}

Automatic differentiation is a tool for automating the evaluation of linear differential
operators on complex functions.  In particular, automatic differentiation techniques
utilize \textit{dual numbers} to factor analytic manipulations, i.e. the computation of
partial derivatives, from the algebraic manipulations that can be programmatically
automated.

In this chapter we review the theory behind linear differential operations, dual numbers,
and automatic differentiation beginning with first-order evaluations and then generalizing 
to higher-order manipulations.

Firstly, a quick note on notation.  Here frakturs refers to dual units, roman letters are 
reserved for real numbers, and greek letters are used for dual numbers.  We consider 
only maps between Euclidean spaces, $f : \mathbb{R}^{N} \rightarrow \mathbb{R}^{M}$ 
for some dimensions $N$ and $M$, with subscripts denoting the coordinates of a point
and bold face denoting the point itself. For example, a many-to-one function,
%
\begin{equation*}
f : \mathbb{R}^{N} \rightarrow \mathbb{R},
\end{equation*}
%
is written as
%
\begin{equation*}
f \! \left( x_{1}, \ldots, x_{N} \right) \equiv f \! \left( \mathbf{x} \right),
\end{equation*} 
%
while the components of a many-to-many function,
%
\begin{equation*}
f : \mathbb{R}^{N} \rightarrow \mathbb{R}^{M},
\end{equation*}
%
are written as
%
\begin{equation*}
f_{m} \! \left( x_{1}, \ldots, x_{N} \right) \equiv f_{m} \! \left( \mathbf{x} \right), \, m \in 1, \ldots, M.
\end{equation*} 

\section{Linear Differential Operations}

Intuitively, linear differential operations quantify the sensitivity of a function, 
$f : \mathbb{R}^{N} \rightarrow \mathbb{R}^{M}$, to the perturbation of its inputs.  
A first-order linear differential operator, for example, maps a function to its \textit{Jacobian},
%
\begin{equation*}
J_{ij} \equiv \frac{ \partial f_{i} }{ \partial x_{j} },
\end{equation*}
%
which then maps a \textit{first-order perturbation} of the inputs, $\delta \mathbf{x}$, 
to a first-order perturbation of the outputs,
%
\begin{align*}
\delta f_{m} 
&=
\sum_{n= 1}^{N} J_{mn} \delta x_{n}
\\
&= 
\sum_{n= 1}^{N} \frac{ \partial f_{m} }{ \partial x_{n} } \delta x_{n}, \, m \in 1, \ldots, M.
\end{align*} 
%
Alternatively we can use the \textit{adjoint Jacobian} to map \textit{first-order sensitivities}
of the outputs into first-order sensitivities of the inputs,
%
\begin{align*}
\mathrm{d} x_{n} 
&= 
\sum_{m = 1}^{M} J^{T}_{nm} \mathrm{d} f_{m}
\\
&=
\sum_{m = 1}^{M} J_{mn} \mathrm{d} f_{m}
\\
&=
\sum_{m = 1}^{M} \frac{ \partial f_{m} }{ \partial x_{n} } \mathrm{d} f_{m}, \, n \in 1, \ldots, N.
\end{align*}

At second-order we have to consider not just second-order perturbations but also the 
contributions arising from the interaction of first-order perturbations.  The resulting 
maps become substantially more cumbersome, such as
%
\begin{equation*}
\delta^{2} f_{m} 
= \sum_{n = 1}^{N} \frac{ \partial f_{m} }{ \partial x_{n} } \delta^{2} x_{n} 
+ \sum_{n = 1}^{N} \sum_{n' = 1}^{N} 
\frac{ \partial^{2} f_{m} }{ \partial x_{n} \partial x_{n'} } 
\delta x_{n} \delta x_{n'}.
\end{equation*}

In practice these operations are incredibly useful because they provide a means of
constructing differential objects, such as direction derivatives, gradients, and Hessians, 
that are ubiquitous in many fields.  For instance, given a many-to-one function,
$f : \mathbb{R}^{N} \rightarrow \mathbb{R}$, the propagation of first-order perturbations
gives a directional derivative,
%
\begin{equation*}
\delta f = \sum_{n = 1}^{N} \frac{ \partial f }{ \partial x_{n} } \delta x_{n},
\end{equation*}
%
while the adjoint mapping of first-order sensitivities yields the components of the gradient,
%
\begin{equation*}
\mathrm{d} x_{i} = \frac{ \partial f }{ \partial x_{i} } \mathrm{d} f.
\end{equation*}

Although conceptually straightforward, the computation and propagation of perturbations and 
sensitivities for complex, composite functions is an arduous and error-prone undertaking, especially
when we consider higher-order operations.  Fortunately, dual numbers implicitly incorporate these 
manipulations and allows for their automation.

\begin{tcolorbox}[colback=gray90,colframe=gray90, coltitle=black,
title=\textbf{Geometric Interpretation of Linear Differential Operations}]

In the previous discussion the difference between perturbations and sensitivities, and 
consequently Jacobians and their adjoints, was subtle, and this can often be
confusing to those unfamiliar with the automatic differentiation literature.  Once we
generalize beyond Euclidean spaces the differences become much more apparent.

\parindent=15pt

Linear differential operations naturally arise on smooth manifolds,
in particular let $\mathcal{M}$ and $\mathcal{N}$ be smooth manifolds with 
$f : \mathcal{M} \rightarrow \mathcal{N}$ a smooth function between them.  Geometrically,
first-order perturbations are elements of the tangent bundle, $T \mathcal{M}$, and the 
pushforward of $f$, $ \left( \mathrm{d} f \right)_{*} : T\mathcal{M} \rightarrow T \mathcal{N}$ 
propagates them to perturbations on $T \mathcal{N}$.  Conversely, first-order sensitivities are
elements of the cotangent bundle, $T^{*} \mathcal{N}$, and the adjoint map that propagates
them to input sensitivities is given by the pull back,
$ \left( \mathrm{d} f \right)^{*} : T^{*} \mathcal{N} \rightarrow T^{*} \mathcal{N}$.

In the special case of a Euclidean space, the manifold, its tangent spaces, and its cotangent
spaces are naturally identified by the canonical metric,
%
\begin{equation*}
\mathcal{M} \cong T \mathcal{M} \cong T^{*} \mathcal{M},
\end{equation*}
%
which explains why the objects appear largely interchangeable.

\end{tcolorbox}

\section{Dual Numbers}

Dual numbers are an extension of the real numbers, $\mathbb{R}$, given by the
addition of a new element, $\mathfrak{a}$, which is nilpotent, $\mathfrak{a}^{2} = 0$,
and denoted a \textit{dual unit}.

This extended space of dual numbers forms a two-dimensional associative algebra 
over the reals.  In other words, the addition of the dual unit generates a two-dimensional
space, $\mathbb{D}$, comprised of elements,
%
\begin{equation*}
\xi = x + \mathfrak{a} \, \delta x; \, \xi \in \mathbb{D}, \, x, \delta x \in \mathbb{R},
\end{equation*}
%
that are naturally equipped with addition and multiplication operations,
%
\begin{align*}
\xi_{1} + \xi_{2} 
&=
 \left( x_{1} + \mathfrak{a} \, \delta x_{1} \right) 
 + \left( x_{2} + \mathfrak{a} \,  \delta x_{2} \right) 
\\
&=
\left( x_{1} + x_{2} \right) 
+ \mathfrak{a} \left( \delta x_{1} + \delta x_{2} \right) 
\\
\xi_{1} \cdot \xi_{2} 
&= 
\left( x_{1} + \mathfrak{a} \, \delta x_{1} \right) 
\cdot \left( x_{2} + \mathfrak{a} \, \delta x_{2} \right) 
\\
&= 
x_{1} x_{2} + \left( x_{1} \delta x_{2} 
+ \delta x_{1} x_{2} \right) \mathfrak{a} 
+ \delta x_{1} \delta x_{2} \, \mathfrak{a}^{2} 
\\
&= 
x_{1} x_{2} + \left( x_{1} \delta x_{2} + \delta x_{1} x_{2} \right) \mathfrak{a}.
\end{align*}  
%
Because the dual unit is nilpotent, we cannot define an inverse to multiplication.  
Mathematically this means that the dual numbers form only a local ring; compare this to 
the complex numbers on which we can define division and whose associative algebra is a field.

Dual numbers have many uses in mathematics and physics, but here we will focus on their
relationship to functions.  Any smooth function $f : \mathbb{R} \rightarrow \mathbb{R}$ 
induces a function on the dual numbers, $f : \mathbb{D} \rightarrow \mathbb{D}$, 
via a Taylor series around any purely real point, $\xi_{0} = x_{0} + \mathfrak{a} \, 0$,
%
\begin{align*}
f \! \left( \xi \right) 
&= 
\sum_{n = 0}^{\infty} \frac{ \left( \xi - \xi_{0} \right)^{n} }{n!} 
\frac{ \partial^{n} f }{ \partial \xi^{n} } \! \left( \xi_{0} \right) 
\\
&= 
\sum_{n = 0}^{\infty} \frac{ \left( \left( x - x_{0} \right) + \mathfrak{a} \, \delta x \right)^{n} }{n!} 
\frac{ \partial^{n} f }{ \partial x^{n} } \! \left( x_{0} \right)
\\
&= 
\sum_{n = 0}^{\infty} \frac{ \left( x - x_{0} \right)^{n} }{n!} 
\frac{ \partial^{n} f }{ \partial x^{n} } \! \left( x_{0} \right)
+ \mathfrak{a} \, \delta x \sum_{n = 0}^{\infty} \frac{ \left( x - x_{0} \right)^{n - 1} }{\left( n - 1 \right)!} 
\frac{ \partial^{n} f }{ \partial x^{n} } \! \left( x_{0} \right) 
\\
&= 
f \! \left( x \right) 
+ \mathfrak{a} \, \delta x \frac{ \partial f }{ \partial x } \! \left( x \right).
\end{align*}
%
The real part of the evaluation yields the value of the function, while the dual part
is exactly the image of the dual input, $\delta x$, under the Jacobian.

\begin{tcolorbox}[colback=gray90,colframe=gray90, coltitle=black,
title=\textbf{Dual Numbers, Exterior Derivatives, and Infinitetesimals}]

Introductory calculus is also taught around the idea of an ``infinitesimal'' number that 
projects out the best linear approximation of a function.  When moving on to the more
rigorous real analysis, however, such concepts become difficult to define.

\parindent=15pt

One possible defining property of an infinitesimal is a number that is small enough that
its square vanishes but not so small that it is indistinguishable form zero itself, a property
that is manifestly captured in nilpotent objects such as the dual unit and the exterior
derivative from differential geometry.  It should be no surprise, then, that dual numbers
and exterior calculus provide a means of formalizing intuitive concepts such as
infinitesimals, tangents, and linear approximations.

\end{tcolorbox}

The generalization to multivariate functions is straightforward: any function 
$f : \mathbb{R}^{N} \rightarrow \mathbb{R}^{M}$ is first decomposed into $M$
functions, $f_{m} : \mathbb{R}^{N} \rightarrow \mathbb{R}$, which generalize
to dual numbers as
%
\begin{equation} \label{dualFunction}
f_{m} \! \left( \mbox{\boldmath{$\xi$}} \right) = f_{m} \! \left( \mathbf{x} \right) 
+ \mathfrak{a} \sum_{n = 1}^{N} 
\delta x_{n} \frac{ \partial f_{m} }{ \partial x_{n} } \! \left( \mathbf{x} \right).
\end{equation}
%
As in the univariate case, the dual part of the output is exactly the result of
mapping the dual part of the input through the Jacobian of $f_{m}$.  

Because dual functions propagate both functions and perturbations, any implementation
of dual functions implicitly automates the intermediate calculations necessary for the 
construction of linear differential operations.

\section{First-Order Automatic Differentiation}

Ultimately, first-order automatic differentiation is just an implementation of dual number-valued
functions.  Implementations become particularly straightforward when we represent functions
with \textit{expression graphs}, with nodes designating dual numbers and edges denoting
functional dependencies (Figure \ref{fig:exprGraph}).  Here the evaluation of a
dual number-valued function reduces to the propagation of dual numbers through the graph.

\begin{figure}
\setlength{\unitlength}{0.1in} 
\centering
\begin{picture}(50, 20)
%
%\put(0, 0) { \framebox(50, 20){} }
%\put(25, 0) { \framebox(25, 30){} }
%\put(25, 0) { \framebox(6.25, 30){} }
%\put(25, 0) { \framebox(12.5, 30){} }
%\put(25, 0) { \framebox(18.75, 30){} }
%
%\put(25, 0) { \framebox(3.125, 30){} }
%\put(25, 0) { \framebox(9.375, 30){} }
%\put(25, 0) { \framebox(15.625, 30){} }
%
\put(12.5, 10) { \makebox(0, 0) 
{$z \! \left( y_{1} \! \left( x_{1}, x_{2} \right), y_{2} \! \left( x_{2}, x_{3} \right) \right)$} }
%
\put(21.875, 10) { \vector(1, 0){6.25} }
%
\put(31.25, 2.5) { \circle{4} }
\put(31.25, 2.5) { \makebox(0, 0) {$ x_{1} $} }
%
\put(37.5, 2.5) { \circle{4} }
\put(37.5, 2.5) { \makebox(0, 0) { $ x_{2} $ } }
%
\put(43.75, 2.5) { \circle{4} }
\put(43.75, 2.5) { \makebox(0, 0) { $ x_{3} $ } }
%
\put(31.25, 4.5) { \vector(3, 4){2.75} }
\put(37.5, 4.5) { \vector(-3, 4){2.75} }
\put(37.5, 4.5) { \vector(3, 4){2.75} }
\put(43.75, 4.5) { \vector(-3, 4){2.75} }
%
\put(35, 10) {\circle{4} } % Tweaked to the right
\put(34.375, 10) { \makebox(0, 0) { $y_{1}$ } }
%
\put(41.25, 10) {\circle{4} } % Tweaked to the right
\put(40.625, 10) { \makebox(0, 0) { $y_{2}$ } }
%
\put(34.375, 12) { \vector(3, 4){2.75} }
\put(40.625, 12) { \vector(-3, 4){2.75} }
%
\put(38, 17.5) {\circle{4} } % Tweaked to the right
\put(37.5, 17.5) { \makebox(0, 0) { $ z $ } }
%
\end{picture} 
\caption{
Composite functions are isomorphic to directed acyclic graphs known as 
an \textit{expression graph} or \textit{expression tree}.  Here we the function
$z \! \left( y_{1} \! \left( x_{1}, x_{2} \right), y_{2} \! \left( x_{2}, x_{3} \right) \right)$
generates a three level graph, with leaves $\left\{ x_{1}, x_{2}, x_{3} \right\}$ and
root $z$.
}
\label{fig:exprGraph} 
\end{figure}

In general the directionality of the propagation is not constrained.  We could,
for example, interpret the dual part of each node differently, with some nodes having
perturbations that propagate forward, from the inputs of the expression graph to the 
outputs and along the function evaluation, and have others having sensitivities propagating 
in reverse, from the outputs to the inputs.  Here we consider only
\textit{forward mode automatic differentiation} where the dual part of every node is
a perturbation, and \textit{reverse mode automatic differentiation} the dual part of every
nodes is a sensitivity.

\subsection{Forward Mode}

In forward mode automatic differentiation, the components of $\delta x_{j}$ defined at 
the inputs of the function, and the leaves of the expression graph, are propagated forward 
through the expression graph using the Jacobian map at each node (Figure \ref{fig:directions}).  
Following the literature we will refer to the components of the dual numbers as \textit{values},
%
\begin{equation*}
\mathcal{V} \! \left( \xi \right) 
= \mathcal{V} \! \left( x + \mathfrak{a} \, \delta x \right) 
= x,
\end{equation*}
%
and \textit{tangents} or \textit{perturbations},
\begin{equation*}
\mathcal{T} \! \left( \xi \right) 
= \mathcal{T} \! \left( x + \mathfrak{a} \, \delta x \right) 
= \delta x,
\end{equation*}
%
respectively.

\begin{figure}
\setlength{\unitlength}{0.1in} 
\centering
\begin{picture}(50, 20)
%
%\put(0, 1) { \framebox(50, 20){} }
%\put(0, 0) { \framebox(12.5, 30){} }
%\put(0, 0) { \framebox(25, 30){} }
%\put(0, 0) { \framebox(37.5, 30){} }
%\put(0, 0) { \framebox(50, 10){} }
%\put(0, 0) { \framebox(50, 20){} }
%
% Forward Mode
%
\put(12.5, 2) { \makebox(0, 0) { Forward Mode } }
%
\put(13, 7) {\circle{4} } % Tweak to the right
\put(12.5, 7) { \makebox(0, 0) { $ \xi_{j} $ } }
%
\put(12, 9) { \vector(0, 1){6} }
\put(13, 9) { \vector(0, 1){6} }
%
\put(13, 17) {\circle{4} } % Tweak to the right
\put(12.5, 17) { \makebox(0, 0) { $ \xi_{i} $ } }
%
\put(6, 12) { \makebox(0, 0) 
{ $ \mathcal{V} \! \left( \xi_{j} \right) = x_{j} \! \left( x_{i} \right)$ } }
\put(16.5, 13) { \makebox(0, 0) 
{ $ \mathcal{T} \! \left( \xi_{i} \right) =  $} }
\put(20, 10) { \makebox(0, 0) 
{ $ \sum_{i} \frac{ \partial x_{i} }{ \partial x_{j} } \mathcal{T} \! \left( \xi_{j} \right)  $ } }
%
% Reverse Mode
%
\put(37.5, 2) { \makebox(0, 0) { Reverse Mode } }
%
\put(38, 7) {\circle{4} } % Tweak to the right
\put(37.5, 7) { \makebox(0, 0) { $ \xi_{j} $ } }
%
\put(37, 9) { \vector(0, 1){6} }
\put(38, 15) { \vector(0, -1){6} }
%
\put(38, 17) {\circle{4} } % Tweak to the right
\put(37.5, 17) { \makebox(0, 0) { $ \xi_{i} $ } }
%
\put(31, 12) { \makebox(0, 0) 
{ $ \mathcal{V} \! \left( x_{j} \right) = x_{j} \! \left( x_{i} \right)$ } }
\put(41.5, 10) { \makebox(0, 0) 
{ $ \mathcal{A} \! \left( \xi_{j} \right) =  $} }
\put(45, 13) { \makebox(0, 0) 
{ $ \sum_{i} \frac{ \partial x_{i} }{ \partial x_{j} } \mathcal{A} \! \left( \xi_{i} \right)  $ } }
%
\end{picture} 
\caption{
In forward mode automatic differentiation the Jacobian propagates directional 
derivatives, $\mathcal{T} \! \left( \xi \right)$ forward in the same direction as the
values, $\mathcal{V} \! \left( \xi \right)$.  Reverse mode automatic differentiation, 
on the other hand, uses the adjoint Jacobian to propagate adjoint directional 
derivatives, $\mathcal{A} \! \left( \xi \right)$, backwards against the 
values, $\mathcal{V} \! \left( \xi \right)$.
}
\label{fig:directions} 
\end{figure}

The propagation of perturbations has a particularly nice interpretation as a 
message passing algorithm \textbf{reference Bishop}.  A node $f_{i}$ with dependent
node $g$, propagates its perturbation forward by emitting the message 
$ \delta f_{i} \partial g / \partial f_{i}$.  Once the node $g$ has received messages from
all of its dependencies, $f_{i}$, it computes its own perturbation, 
$\sum_{i} \delta f_{i} \partial g / \partial f_{i}$, and then emits to any dependent nodes.

\subsection{Reverse Mode}

Reverse mode automatic differentiation propagates sensitivities defined at the outputs,
and the roots of the expression graph, backwards towards to the leaves by applying
the adjoint Jacobian map (Figure \ref{fig:directions}).  In this case the components of 
the dual numbers as denoted \textit{values},
%
\begin{equation*}
\mathcal{V} \! \left( \xi \right) 
= \mathcal{V} \! \left( x + \mathfrak{a} \, \mathrm{d} x \right) 
= x,
\end{equation*}
%
and \textit{adjoints} or \textit{sensitivities},
\begin{equation*}
\mathcal{A} \! \left( \xi \right) 
= \mathcal{A} \! \left( x + \mathfrak{a} \, \mathrm{d} x \right) 
= \mathrm{d} x,
\end{equation*}
%
respectively.

As in forward mode, reverse mode has a nice interpretation as a message passing
algorithm, only with messages emitted and received in the opposite direction.

\subsection{Performance}

The relative performance of forward and reverse mode automatic differentiation
depends on the dimensionality of the inputs and the outputs of the given function
and the linear differential operation being considered.  Note that per evaluation forward 
mode will always be faster than reverse mode as it requires only one forward sweep 
to propagate values and perturbations, compared to the two sweeps, one forward to 
sweep to propagate values and one reverse sweep to propagate sensitivities, required 
for reverse mode.  Moreover the overhead for forward mode will be smaller since it 
does not require the full expression graph to be stored in memory.

Consider, for example, the many-to-one case, $f : \mathbb{R}^{N} \rightarrow \mathbb{R}$.  
Here forward mode computes the directional derivative, $\sum_{k} \delta x_{i} \partial f / \partial x_{i}$,
where reverse mode computes the components of the gradient, $\mathrm{d} f \partial f / \partial x_{i}$.  
If the directional derivative is all that is necessary then the forward mode calculation 
will be quicker given the considerations above.  On the other hand, if the full vector gradient 
is required then the small overhead from reverse mode will be dwarfed by the $M$ repetitions 
necessary to build the full gradient from directional derivatives in forward mode.

In general, computing the full Jacobian is faster with forward mode if $n \ll m$ 
and faster with reverse mode if $n \gg m$.

\section{Higher-Order Automatic Differentiation}

A significant advantage of dual numbers is that they immediately generalize to provide
an implementation of higher-order automatic differentiation techniques.  Such a
generalization requires the introduction of multiple dual units and higher-order dual
numbers.

As we saw above, the real and dual parts of a first-order dual number are simply real 
numbers,
%
\begin{equation*}
\zeta = z + \mathfrak{a} \, \delta z, \, z, \delta z \in \mathbb{R}.
\end{equation*}
%
If we introduce a second dual unit, $\mathfrak{b}$, then we can construct a second-order 
dual number by replacing the real-valued parts of $\zeta$ with dual numbers,
%
\begin{alignat*}{3}
\zeta
&=
\;\, \xi
&&+ \mathfrak{a} \;\;\, \eta
\\
&=
\left( x + \mathfrak{b} \, \delta x \right)
&&+ \mathfrak{a} \left( \delta y + \mathfrak{b} \, \delta^{2} y \right), 
\, x, \delta x, \delta y, \delta^{2} y \in \mathbb{R}.
\end{alignat*}
%
From the forward mode perspective, $x$ is the value, $\delta x$ and $\delta y$ are 
first-order perturbations, and $\delta^{2} y$ is a second-order perturbation.  In order
to remain agnostic to the choice of forward or reverse mode, however, we will refer
to $x$ as the first value, $\delta x$ as the first gradient, $\delta y$ as the second
value, and $\delta^{2}$ as the second gradient.

Higher-order dual numbers follow recursively by replacing real-valued components 
with dual numbers and new dual units.  An $n$-th order dual number, for example, 
is constructed from $2^{n}$ real components.

Implementing higher-order dual number-valued functions is simply a matter of
replacing each node in the expression graph with a higher-order dual number,
and deriving generalizations of the propagation rules.   The resulting forward and 
reverse mode propagations yield higher-order linear differential operations.

Here we evaluate second and third-order dual number-valued functions, and derive the corresponding
propagation rules, by evaluating the function recursively through the dual numbers.  Given the
propagation rules we show how to construct common higher-order linear differential operations,
such as the Hessian.
 
\subsection{Second-Order}

Given the second-order dual number,
%
\begin{alignat*}{3}
\zeta_{i} 
&=
\xi_{i} 
&&+ \mathfrak{a} \, \eta_{i}
\\
&=
\left( x_{i} + \mathfrak{b} \, \delta x_{i} \right)
&&+ \mathfrak{a} \left( \delta y_{i} + \mathfrak{b} \, \delta^{2} y_{i} \right),
\end{alignat*}
%
a second-order dual number valued-function, evaluates to
%
\begin{align*}
f_{i} \! \left(  \mbox{\boldmath{$\zeta$}} \right)
=& \,
f_{i} \! \left(  \mbox{\boldmath{$\xi$}} \right) 
+ \mathfrak{a} \sum_{j} \eta_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left(  \mbox{\boldmath{$\xi$}} \right) 
\\
=& \,
f_{i} \! \left( \mathbf{x} + \mathfrak{b} \, \delta \mathbf{x} \right) 
+ \mathfrak{a} \sum_{j} \left( \delta y_{j} + \mathfrak{b} \, \delta^{2} y_{j} \right)
\frac{ \partial f_{i} }{ \partial x_{j} } 
\! \left( \mathbf{x} + \mathfrak{b} \, \delta \mathbf{x} \right) 
\\
=& \,
f_{i} \! \left( \mathbf{x} \right) 
+ \mathfrak{b} \sum_{j} \delta x_{j}  \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x} \right)
\\
&+ 
\mathfrak{a} \sum_{j} \left( \delta y_{j} + \mathfrak{b} \, \delta^{2} y_{j} \right)
\left( \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x}\right) + 
\mathfrak{b} \, \sum_{k} \delta x_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{x} \right) \right) 
\\
=&
\quad\quad\quad 
f_{i} \! \left( \mathbf{x} \right) 
\quad\quad\quad\;\;\;
+ \mathfrak{b} \;\;\;
\sum_{j} \delta x_{j}  \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x} \right)
\\
&+ 
\mathfrak{a} \left( 
\sum_{j} \delta y_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x} \right) 
+ \mathfrak{b} \left(
\sum_{j} \delta^{2} y_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x} \right)
+ \sum_{jk} \delta x_{k} \, \delta y_{j}
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{x} \right)
\right)
\right)
\end{align*}
%
These results are summarized in Table \ref{tab:secondOrder}.

\begin{table*}[t!]
	\centering
	\renewcommand{\arraystretch}{2}
	\begin{tabular}{ccc}
	\rowcolor[gray]{0.9} \textbf{Component} & \textbf{Input} & \textbf{Output} \\
	First Value & 
	$\mathbf{x}$ & 
	$f_{i} \! \left( \mathbf{x} \right)$ 
	\\
	\rowcolor[gray]{0.9} 
	First Gradient & 
	$\delta \mathbf{x}$ &
	$\displaystyle \sum_{j} \delta x_{j}  \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x} \right)$
	\\
	Second Value & 
	$\delta \mathbf{y} $ & 
	$\displaystyle \sum_{j} \delta y_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x} \right)$
	\\
	\rowcolor[gray]{0.9} 
	Second Gradient & 
	$\delta^{2} \mathbf{y} $ & 
	$\displaystyle \sum_{j} \delta^{2} y_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{x} \right)
	+ \sum_{jk} \delta x_{k} \, \delta y_{j}
	\frac{ \partial f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{x} \right)$
	\\
	\end{tabular}
	\caption{Recursively expanding a function with respect to a second-order dual number 
	gives the values of each component of the dual number.
	\label{tab:secondOrder}}
\end{table*}

In forward mode we compute four different values at each node -- the function
evaluation, two first-order perturbations, and the second-order perturbation,
$ \sum_{j} \delta^{2} y_{j} \, \partial f_{i} / \partial x_{j}
+ \sum_{jk} \delta x_{k} \, \delta y_{j} \, \partial f_{i} / \partial x_{j} \partial x_{k}$.

The generalization of reverse mode requires some care because the second-order
Jacobian, $\partial f_{i} / \partial x_{j} \partial x_{k}$, does not have a well-defined
adjoint.  If we propagate the second-order values forward first, however,
then we can define the adjoint Jacobian operator as
%
\begin{equation*}
J^{T}_{ij} = \sum_{k} \delta y_{k} \, \partial f_{i} / \partial x_{k} \partial x_{j},
\end{equation*}
%
and the resulting propagation of sensitivities as
%
\begin{align*}
\mathrm{d} x_{i} 
&= 
\sum_{j} \mathrm{d} f_{j} J^{T}_{ij} 
\\
&= 
\sum_{jk} \mathrm{d} f_{j} \, \delta y_{k} \, \partial f_{i} / \partial x_{k} \partial x_{j}.
\end{align*}

Second-order reverse mode then consists of a forward sweep in which the first and 
second-order values are computed, and then reverse sweep in which the first and 
second-order gradients are computed.  This  yields the function evaluation, the 
sensitivities of the inputs with respect to $\mathrm{d} \mathbf{f}$, the perturbation of the 
output with respect to $\delta \mathbf{y}$, and the second-order sensitivities,
%
\begin{equation*}
\sum_{j} \mathrm{d}^{2} y_{j} \, \partial f_{i} / \partial x_{j}
+ \sum_{jk} \mathrm{d} f_{j} \, \delta y_{k} \, \partial f_{i} / \partial x_{j} \partial x_{k}.
\end{equation*}

\subsection{Third-Order}

Continuing to third-order follows in kind.  Given a third-order dual number,
%
\begin{alignat*}{10}
\zeta_{i} 
&=
&& \xi_{i} && && &&
&&+ \mathfrak{a} \, 
&& \eta_{i} && && &&
\\
&=
( && \sigma_{i} && + \mathfrak{b} && \tau_{i} &&)
&&+ \mathfrak{a} \,
( && \upsilon_{i} && + \mathfrak{b} && \nu_{i} && )
\\
&=
(( && s_{i} + \mathfrak{c} \, \delta s_{i} )
&& + \mathfrak{b} \;
( && \delta t_{i} + \mathfrak{c} \, \delta^{2} t_{i}  ) &&)
&&+ \mathfrak{a} \;
(( && \delta u_{i} + \mathfrak{c} \, \delta^{2} u_{i} )
&& + \mathfrak{b} \;
( && \delta^{2} v_{i} + \mathfrak{c} \, \delta^{3} v_{i} ) &&)
\end{alignat*}
%
we have
%
\begin{align*}
f_{i} \! \left( \mbox{\boldmath{$\zeta$}} \right)
%
=& \,
f_{i} \! \left( \mbox{\boldmath{$\xi$}}\right) 
+ \mathfrak{a} \sum_{j} \eta_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mbox{\boldmath{$\xi$}} \right) 
\\
%
=& \,
f_{i} \! \left( \mbox{\boldmath{$\sigma$}} + \mathfrak{b} \, \mbox{\boldmath{$\tau$}} \right) 
+ \mathfrak{a} \sum_{j} \left( \upsilon_{j} + \mathfrak{b} \, \nu_{j} \right)
\frac{ \partial f_{i} }{ \partial x_{j} } 
\! \left( \mbox{\boldmath{$\sigma$}} + \mathfrak{b} \, \mbox{\boldmath{$\tau$}} \right) 
\\
%
=& \,
f_{i} \! \left( \mbox{\boldmath{$\sigma$}} \right) + \mathfrak{b} 
\sum_{j} \tau_{j} \frac{ \partial f_{i} }{ \partial x_{j} } 
\! \left( \mbox{\boldmath{$\sigma$}} \right)
+ \mathfrak{a} \sum_{j} \left( \upsilon_{j} + \mathfrak{b} \, \nu_{j} \right)
\left( \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mbox{\boldmath{$\sigma$}} \right)
+ \mathfrak{b} \sum_{k} \tau_{k}
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mbox{\boldmath{$\sigma$}} \right) \right) 
\\
%
=& \,
f_{i} \! \left( \mbox{\boldmath{$\sigma$}} \right) + \mathfrak{b} 
\sum_{j} \tau_{j} \frac{ \partial f_{i} }{ \partial x_{j} } 
\! \left( \mbox{\boldmath{$\sigma$}} \right)
+ \mathfrak{a} \sum_{j}
\upsilon_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mbox{\boldmath{$\sigma$}} \right)
\\
& + \mathfrak{a} \, \mathfrak{b} \sum_{j}
 \nu_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mbox{\boldmath{$\sigma$}} \right)
+ \mathfrak{a} \, \mathfrak{b} \, \sum_{jk} \upsilon_{j} \tau_{k}
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mbox{\boldmath{$\sigma$}} \right)
\\
%
=& \,
f_{i} \! \left( \mathbf{s} + \mathfrak{c} \, \delta \mathbf{s} \right) + \mathfrak{b} 
\sum_{j} \left( \delta t_{j} + \mathfrak{c} \, \delta^{2} t_{j} \right) 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} + \mathfrak{c} \, \delta \mathbf{s}\right)
\\
&+ 
\mathfrak{a} \sum_{j}
\left( \delta u_{j} + \mathfrak{c} \, \delta^{2} u_{j} \right) 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} + \mathfrak{c} \, \delta \mathbf{s}\right)
\\
& + \mathfrak{a} \, \mathfrak{b} \sum_{j}
\left( \delta^{2} v_{j} + \mathfrak{c} \, \delta^{3} v_{j} \right) 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} + \mathfrak{c} \, \delta \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \, \sum_{jk} 
\left( \delta u_{j} + \mathfrak{c} \, \delta^{2} u_{j} \right) 
\left( \delta t_{k} + \mathfrak{c} \, \delta^{2} t_{k} \right)
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} + \mathfrak{c} \, \delta \mathbf{s} \right)
\\
%
=& \,
f_{i} \! \left( \mathbf{s} \right) + \mathfrak{c} \sum_{j} \delta s_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{b}
\sum_{j} \left( \delta t_{j} + \mathfrak{c} \, \delta^{2} t_{j} \right) 
\left( \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+  \mathfrak{c} \sum_{k} \delta s_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) \right)
\\
&+ 
\mathfrak{a} \sum_{j}
\left( \delta u_{j} + \mathfrak{c} \, \delta^{2} u_{j} \right) 
\left( \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{c} \sum_{k} \delta s_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) \right)
\\
& + \mathfrak{a} \, \mathfrak{b} \sum_{j}
\left( \delta^{2} v_{j} + \mathfrak{c} \, \delta^{3} v_{j} \right) 
\left( \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{c} \sum_{k} \delta s_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \, \sum_{jk} 
\left( \delta u_{j} + \mathfrak{c} \, \delta^{2} u_{j} \right) 
\left( \delta t_{k} + \mathfrak{c} \, \delta^{2} t_{k} \right)
\\
& \quad\quad\quad\quad \times \left(
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) 
+ \mathfrak{c} \sum_{l} \delta s_{l} 
\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( \mathbf{s} \right)
\right)
\\
%
\end{align*}

\begin{align*}
f_{i} \! \left( \mbox{\boldmath{$\zeta$}} \right)
%
=& \,
f_{i} \! \left( s_{j} \right) + \mathfrak{c} \sum_{j} \delta s_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{b} \sum_{j} \delta t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{2} t_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{b} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta t_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \sum_{j} \delta u_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{a} \, \mathfrak{c} \sum_{j} \delta^{2} u_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{a} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta u_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \sum_{j} \delta^{2} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{3} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta^{2} v_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \, \sum_{jk} 
\left( \delta u_{j} \delta t_{k}
+ \mathfrak{c} \left(
\delta u_{j} \, \delta^{2} t_{k} + \delta^{2} u_{j} \, \delta t_{k} 
\right) \right)
\\
& \quad\quad\quad\quad \times \left(
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) 
+ \mathfrak{c} \sum_{l} \delta s_{l} 
\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( \mathbf{s} \right)
\right)
\\
%
=& \,
f_{i} \! \left( s_{j} \right) + \mathfrak{c} \sum_{j} \delta s_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{b} \sum_{j} \delta t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{2} t_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{b} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta t_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \sum_{j} \delta u_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{a} \, \mathfrak{c} \sum_{j} \delta^{2} u_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{a} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta u_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \sum_{j} \delta^{2} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{3} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta^{2} v_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} \, \sum_{jk} 
\delta u_{j} \delta t_{k} \frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jk}
\left( \delta u_{j} \, \delta^{2} t_{k} + \delta^{2} u_{j} \, \delta t_{k}  \right)
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) 
\\
&+
\mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jkl}
\delta u_{j} \delta t_{k} \, \delta s_{l} 
\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( \mathbf{s} \right)
\\
%
\end{align*}

\begin{align*}
f_{i} \! \left( \mbox{\boldmath{$\zeta$}} \right)
%
=& \,
f_{i} \! \left( s_{j} \right) + \mathfrak{c} \sum_{j} \delta s_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{b} \sum_{j} \delta t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{2} t_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{b} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta t_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \sum_{j} \delta u_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{a} \, \mathfrak{c} \sum_{j} \delta^{2} u_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \mathfrak{a} \, \mathfrak{c} \sum_{jk} \delta s_{k} \, \delta u_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
&+ 
\mathfrak{a} \, \mathfrak{b} 
\left( 
\sum_{j} \delta^{2} v_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
\delta u_{j} \delta t_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\right)
\\
&+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{j} \delta^{3} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
\\
&+ \mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jk}
\left( 
\delta s_{k} \, \delta^{2} v_{j} + \delta u_{j} \, \delta^{2} t_{k} + \delta^{2} u_{j} \, \delta t_{k}  \right)
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) 
\\
&+
\mathfrak{a} \, \mathfrak{b} \, \mathfrak{c} \sum_{jkl}
\delta u_{j} \delta t_{k} \, \delta s_{l} 
\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( \mathbf{s} \right)
\\
%
=& \hspace{26mm} 
f_{i} \! \left( \mathbf{s} \right) 
\hspace{9mm}
+ \mathfrak{c} 
\hspace{3mm}
\sum_{j} \delta s_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
\\
& \hspace{8mm} + 
\mathfrak{b} \left( \sum_{j} \delta t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
\hspace{1mm}
+ \mathfrak{c} \left( 
\sum_{j} \delta^{2} t_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \sum_{jk} \delta s_{k} \, \delta t_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) \right) \right)
\\
&+ 
\mathfrak{a} \Bigg( 
\hspace{9mm}
\sum_{j} \delta u_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
+ \mathfrak{c} \left( \sum_{j} \delta^{2} u_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
+ \sum_{jk} \delta s_{k} \, \delta u_{j} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) \right)
\\
& 
\hspace{8mm} 
+ \mathfrak{b} \Bigg(
\hspace{0.5mm}
\sum_{j} \delta^{2} v_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
\delta u_{j} \delta t_{k} 
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)
\\
& \hspace{34mm} 
+ \mathfrak{c} \Bigg(
\sum_{j} \delta^{3} v_{j} 
\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
\\
& \hspace{40mm} 
+ \sum_{jk}
\left( 
\delta s_{k} \, \delta^{2} v_{j} + \delta u_{j} \, \delta^{2} t_{k} + \delta^{2} u_{j} \, \delta t_{k}  \right)
\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) 
\\
& \hspace{40mm} +
\sum_{jkl}
\delta u_{j} \delta t_{k} \, \delta s_{l} 
\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( \mathbf{s} \right) 
\Bigg)
\Bigg)
\Bigg)
\\
%
\end{align*}
%
These results are summarized in Table \ref{tab:thirdOrder}.

\begin{table*}[t!]
	\centering
	\renewcommand{\arraystretch}{2}
	\begin{tabular}{ccc}
	\rowcolor[gray]{0.9} \textbf{Component} & \textbf{Input} & \textbf{Output} \\
	First Value & 
	$\mathbf{s}$ & 
	$f_{i} \! \left( \mathbf{s} \right)$ 
	\\
	\rowcolor[gray]{0.9} 
	First Gradient & 
	$\delta \mathbf{s} $ &
	$\displaystyle \sum_{j} \delta s_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) $
	\\
	Second Value & 
	$\delta \mathbf{t}$ & 
	$\displaystyle \sum_{j} \delta t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)$
	\\
	\rowcolor[gray]{0.9} 
	Second Gradient & 
	$\delta^{2} \mathbf{t}$ &
	$\displaystyle \sum_{j} \delta^{2} t_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
	+ \sum_{jk} \delta s_{k} \, \delta t_{j} 
	\frac{ \partial f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)$
	\\
	Third Value & 
	$\delta \mathbf{u}$ &
	$\displaystyle \sum_{j} \delta u_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)$
	\\
	\rowcolor[gray]{0.9} 
	Third Gradient & 
	$\delta^{2} \mathbf{u}$ &
	$\displaystyle \sum_{j} \delta^{2} u_{j} 
	\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) 
	+ \sum_{jk} \delta s_{k} \, \delta u_{j} 
	\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)$
	\\
	Fourth Value & 
	$\delta^{2} \mathbf{v}$ & 
	$\displaystyle \sum_{j} \delta^{2} v_{j} \frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right)
	\delta u_{j} \delta t_{k} 
	\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right)$
	\\
	\rowcolor[gray]{0.9} 
   \multirow{3}{*}{ \vspace{-8mm} Fourth Gradient} & 
	\multirow{3}{*}{ \vspace{-8mm} $\delta^{3} \mathbf{v}$} &
	$\displaystyle \sum_{j} \delta^{3} v_{j} 
	\frac{ \partial f_{i} }{ \partial x_{j} } \! \left( \mathbf{s} \right) $ 
	\\
	& &
	$\displaystyle + \sum_{jk} \left( 
	\delta s_{k} \, \delta^{2} v_{j} + \delta u_{j} \, \delta^{2} t_{k} + \delta^{2} u_{j} \, \delta t_{k}  \right)
	\frac{ \partial^{2} f_{i} }{ \partial x_{j} \partial x_{k} } \! \left( \mathbf{s} \right) $ 
	\\
	\rowcolor[gray]{0.9} 
	& &
	$ \displaystyle+ \sum_{jkl} \delta u_{j} \delta t_{k} \, \delta s_{l} 
	\frac{ \partial^{3} f_{i} }{ \partial x_{j} \partial x_{k} \partial_{l} } \! \left( \mathbf{s} \right) $
	\\
	\end{tabular}
	\caption{Recursively expanding a function with respect to a third-order dual number 
	gives the values of each component of the dual number.
	\label{tab:thirdOrder}}
\end{table*}

A forward sweep of third-order dual numbers yields a wealth of information.  In addition
to the function evaluation we have three different first-order perturbations, three different 
perturbations, and a single third-order perturbation.

As above, reverse mode requires a separation of values and gradients.  A
preliminary forwards sweep first builds the expression graph and computes the
function, three first-order perturbations, and a second-order perturbation.  Given 
the perturbations we can define adjoint Jacobians and sweep backwards, generating 
a first-order sensitivity, two second-order sensitivities, and one third-order sensitivity.

\subsection{Constructing Linear Differential Operations}

Given the higher-order propagation methods we can now construct various linear 
differential operations by propagating perturbations and sensitivities.  For simplicity
we consider only many-to-one functions, $f : \mathbb{R}^{N} \rightarrow 1$, as
many-to-many functions can always be decomposed into many-to-one functions.

We denote the gradient by $\mathbf{g}$ and the Hessian as $\mathbf{H}$.

Note that adjuncts refers to superfluous linear differential operations that are generated 
in the construction of the desired operation.

\begin{tcolorbox}[colback=white,colframe=gray90, coltitle=black,boxrule=3pt,
fonttitle=\bfseries,title=Directional Derivative]

	\begin{description}
		\item[Form:] $\displaystyle \mathbf{v}^{T} \mathbf{g} = \sum_{i} v_{i} \frac{ \partial f }{ \partial x_{i} } $
		\item[Algorithm:] \hfill \\
		Initialize $\delta x_{i} = v_{i}$. \\
		Compute first-order forward sweep. \\
		Return first gradient of output.
		\item[Cost:] $\mathcal{O} \! \left( 1 \right)$
		\item[Adjuncts:] $ f $
	\end{description}
	
\end{tcolorbox}

\begin{tcolorbox}[colback=white,colframe=gray90, coltitle=black,boxrule=3pt,
fonttitle=\bfseries,title=Gradient]
	
	\begin{description}
		\item[Form:] $\displaystyle g_{i} = \frac{ \partial f }{ \partial x_{i} } $
		\item[Algorithm:] \hfill \\
		Initialize $\mathrm{d} f = 1$. \\
		Compute first-order reverse sweep. \\
		Return first gradient of inputs.
		\item[Cost:] $\mathcal{O} \! \left( 1 \right)$
		\item[Adjuncts:] $ f $
	\end{description}
	
\end{tcolorbox}

\begin{tcolorbox}[colback=white,colframe=gray90, coltitle=black,boxrule=3pt,
fonttitle=\bfseries,title=Hessian Quadratic Form]

	\begin{description}
		\item[Form:] 
		$\displaystyle \mathbf{v}^{T} \mathbf{H} \, \mathbf{u} = \sum_{ij} v_{i} u_{j} \frac{ \partial^{2} f }{ \partial x_{i} \partial x_{j} } $
		\item[Algorithm:] \hfill \\
		Initialize $\delta x_{i} = v_{i}, \, \delta y_{i} = u_{i}, \delta^{2} y_{i} = 0$. \\
		Compute second-order forward sweep. \\
		Return second gradient of output.
		\item[Cost:] $\mathcal{O} \! \left( 1 \right)$
		\item[Adjuncts:] $ f, \, \mathbf{v}^{T} \mathbf{g}, \, \mathbf{u}^{T} \mathbf{g}$
	\end{description}
	
\end{tcolorbox}

\begin{tcolorbox}[colback=white,colframe=gray90, coltitle=black,boxrule=3pt,
fonttitle=\bfseries,title=Hessian-Vector Product]
	
	\begin{description}
		\item[Form:] $\displaystyle \mathbf{H} \, \mathbf{v} = \sum_{j} v_{j} \frac{ \partial^{2} f }{ \partial x_{i} \partial x_{j} } $
		\item[Algorithm:] \hfill \\
		Initialize $\mathrm{d} f = 1$. \\
		Compute first-order reverse sweep for first gradients. \\
		Initialize $\delta y_{i} = v_{i}$. \\
		Propagate second values in a forward sweep. \\
		Initialize $\mathrm{d}^{2} f = 0.$ \\
		Compute second-order reverse sweep. \\
		Return second gradient of inputs.
		\item[Cost:] $\mathcal{O} \! \left( 1 \right)$
		\item[Adjuncts:] $ f, \, \mathbf{v}^{T} \mathbf{g}, \, \mathbf{g}$
	\end{description}
	
\end{tcolorbox}

\begin{tcolorbox}[colback=white,colframe=gray90, coltitle=black,boxrule=3pt,
fonttitle=\bfseries,title=Hessian]
	
	\begin{description}
		\item[Form:] $\displaystyle H_{ij} = \frac{ \partial^{2} f }{ \partial x_{i} \partial x_{j} } $
		\item[Algorithm:] \hfill \\
		Initialize $\mathrm{d} f = 1$. \\
		Compute first-order reverse sweep for first gradients. \\
		For each $j$ compute the $j$th row of the Hessian as:
		\begin{itemize}
			\setlength{\itemsep}{0cm}
			\setlength{\parskip}{0cm}
			\item[] Initialize $\delta y_{i} = \delta^{j}_{i}$.
			\item[] Propagate second values in a forward sweep. 
			\item[] Initialize $\mathrm{d}^{2} f = 0.$
			\item[] Compute second-order reverse sweep.
			\item[] Return second gradient of inputs
		\end{itemize}
		\item[Cost:] $\mathcal{O} \! \left( n \right)$
		\item[Adjuncts:] $ f, \, \mathbf{g}$
	\end{description}
	
\end{tcolorbox}

\begin{tcolorbox}[colback=white,colframe=gray90, coltitle=black,boxrule=3pt,
fonttitle=\bfseries,title=Gradient of the Trace of a Matrix Hessian Product]
	
	\begin{description}
		\item[Form:] 
		$\displaystyle \frac{\partial}{\partial x_{i} } \mathrm{Tr} \! \left[ \mathbf{M} \, \mathbf{H} \right]
		= \sum_{jk} M_{jk} \frac{ \partial^{3} f }{ \partial x_{i}  \partial x_{j}  \partial x_{k} } $
		\item[Algorithm:] \hfill \\
		Initialize $\mathrm{d} f = 1$. \\
		Compute first-order reverse sweep for first gradients. \\
		Initialize the trace gradient to zero.
		For each $j$ increment the trace gradient with:
		\begin{itemize}
			\setlength{\itemsep}{0cm}
			\setlength{\parskip}{0cm}
			\item[] Initialize $\delta t_{i} = \delta^{j}_{i}$.
			\item[] Propagate second values in a forward sweep. 
			\item[] Initialize $\mathrm{d}^{2} f = 0$.
			\item[] Compute second-order reverse sweep.
			\item[] Initialize $\delta u_{i} = M_{ji}, \delta^{2} v_{i} = 0$.
			\item[] Propagate third and fourth values in a  forward sweep.
			\item[] Initialize $\mathrm{d}^{3} f = 0$.
			\item[] Compute third-order reverse sweep.
			\item[] Return the fourth gradient of the inputs.
		\end{itemize}
		\item[Cost:] $\mathcal{O} \! \left( n \right)$
		\item[Adjuncts:] $ f, \, \mathbf{g}, \, \mathbf{H}$
	\end{description}
	
\end{tcolorbox}
